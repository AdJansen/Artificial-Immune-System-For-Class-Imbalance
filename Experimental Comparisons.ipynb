{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, cross_validate\n",
    "\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#multiclass imports\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier #Will identify the maority calss base line, model needs to do better then the baseline\n",
    "\n",
    "from statistics import mean\n",
    "# to reduce randomness then you put the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "from ArtificialImmuneSystem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\GeneratedSyntheticData-NoiselessInformativeEasy.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: \n",
      "(300, 6)\n",
      "\n",
      "Data size: \n",
      "1800\n",
      "\n",
      "Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Old Class Distribution: Counter({0.0: 210, 1.0: 90})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Data size: \\n{df.size}\\n\")\n",
    "print(f\"Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Old Class Distribution: {Counter(df['5'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled Data shape: \n",
      "(300, 6)\n",
      "\n",
      "Oversampled Data size: \n",
      "1800\n",
      "\n",
      "Oversampled Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "New Class Distribution: Counter({0.0: 210, 1.0: 210})\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create an oversampling object\n",
    "oversample = SMOTE()\n",
    "#Oversample and add to the dataframe to fix the class imbalance\n",
    "x_over, y_over = oversample.fit_resample(df.drop([\"5\"], axis=1), df.drop(df.columns[0:-1],axis=1))\n",
    "smote_df = pd.concat([x_over, y_over], axis=1)\n",
    "\n",
    "# print the dimensionality of the oversampled dataset\n",
    "print(f\"Oversampled Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Oversampled Data size: \\n{df.size}\\n\")\n",
    "print(f\"Oversampled Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"New Class Distribution: {Counter(smote_df['5'])}\")\n",
    "\n",
    "print(\"_____________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aisOversample = ArtificialImmuneSystem()\n",
    "#minority_class = df[df['5'] == 1]\n",
    "#majority_class = df[df['5'] == 0]\n",
    "\n",
    "#requiredPopulation = len(majority_class)-len(minority_class)\n",
    "#population = aisOversample.AIS(minority_class, max_rounds=100, totalPopulation=requiredPopulation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "            0         1         2         3         4    5\n",
      "232 -4.691545 -4.897106 -7.574577 -6.690183 -4.943022  0.0\n",
      "59  -5.540908 -3.826580 -5.718433 -5.464299 -7.812865  0.0\n",
      "6   -5.558902 -4.469969 -4.521977 -1.187704 -3.489468  0.0\n",
      "185  3.530166  6.453537  5.518635 -5.984399 -3.861131  1.0\n",
      "173 -3.874927 -4.519557 -6.019036 -5.568689 -4.379450  0.0\n",
      "..        ...       ...       ...       ...       ...  ...\n",
      "188 -4.382753 -4.156182 -4.945608 -4.251797 -4.573749  0.0\n",
      "71  -4.917879 -5.336254 -5.289513 -8.989536 -6.432483  0.0\n",
      "106  4.376757  7.063312  6.773084 -5.074527 -4.982809  1.0\n",
      "270 -5.132905 -5.448679 -3.954188 -3.423394 -2.365563  0.0\n",
      "102 -5.199458 -4.936755 -6.149472 -5.588318 -4.449639  0.0\n",
      "\n",
      "[240 rows x 6 columns]\n",
      "\n",
      "Test Data shape: \n",
      "            0         1         2         3         4    5\n",
      "203 -5.452907 -4.913225 -3.880859 -4.158012 -4.186854  0.0\n",
      "266 -5.466881 -6.877064 -6.090096 -4.461881 -4.315136  0.0\n",
      "152  4.699258  5.361881  6.027668 -4.969582 -4.990885  1.0\n",
      "9   -5.010238 -2.273145 -6.408167 -6.647583 -8.817610  0.0\n",
      "233  4.901756  4.938506  4.737213 -5.158632 -4.912881  1.0\n",
      "226  5.294774  3.697034  4.836675 -7.587754 -5.021268  1.0\n",
      "196  5.543394  3.381955  3.779498 -3.263850 -5.624578  1.0\n",
      "109  4.688117  5.969182  6.374644 -7.082709 -4.919892  1.0\n",
      "5   -5.722621 -5.869430 -5.060552 -3.792765 -3.505551  0.0\n",
      "175 -5.592691 -4.958004 -5.226683 -5.225733 -5.440950  0.0\n",
      "237  4.583797  7.546729  5.999683 -6.256342 -4.177344  1.0\n",
      "57  -4.961842 -4.873437 -5.070948 -6.117641 -5.689181  0.0\n",
      "218 -5.888881 -5.526152 -5.815783 -5.955422 -5.524494  0.0\n",
      "45  -5.487388 -3.013314 -6.009145 -3.250708 -3.947677  0.0\n",
      "182 -4.939837 -2.376040 -6.295114 -5.119049 -6.317065  0.0\n",
      "221 -6.447492 -4.088617 -6.404756 -3.276425 -5.473491  0.0\n",
      "289 -5.058468 -5.287388 -4.698604 -5.897789 -6.716347  0.0\n",
      "211 -5.574853 -4.550226 -4.400662 -5.140022 -4.908897  0.0\n",
      "148 -3.749611 -4.789767 -5.245193 -7.341026 -6.314782  0.0\n",
      "165 -5.376927 -4.819349 -3.791505 -4.480134 -4.539707  0.0\n",
      "78  -5.130252 -6.467679 -3.782191 -8.058004 -6.472565  0.0\n",
      "113  3.402719  5.003821  4.350144 -5.227083 -3.889555  1.0\n",
      "249 -4.849378 -4.553499 -4.111270 -5.061952 -4.055680  0.0\n",
      "250  5.593270  4.364744  3.769207 -5.521032 -4.824245  1.0\n",
      "104 -4.785916 -5.874768 -4.320362 -6.083843 -5.359186  0.0\n",
      "42   5.617058  5.288188  6.531158 -5.066146 -6.087117  1.0\n",
      "281  4.103299  3.925165  4.947911 -4.557117 -4.567186  1.0\n",
      "295 -4.757174 -6.271916 -5.165861 -5.624757 -4.289127  0.0\n",
      "157 -4.666674 -4.085056 -4.458655 -6.411579 -6.717503  0.0\n",
      "238 -3.508291 -3.783912 -5.245605 -5.845384 -4.929951  0.0\n",
      "17   4.574268  5.343613  4.097700 -5.448176 -4.215139  1.0\n",
      "164 -5.292489 -4.086846 -4.133240 -3.920018 -6.010868  0.0\n",
      "33  -5.117716 -5.746916 -4.387790 -4.654688 -4.485620  0.0\n",
      "24   5.083777  5.484801  5.384913 -4.399167 -5.478308  1.0\n",
      "215  6.229801  4.051011  3.094323 -5.756406 -5.123341  1.0\n",
      "119 -5.084994 -3.889805 -4.923373 -1.650257 -3.232784  0.0\n",
      "7   -3.714469 -4.916281 -4.678875 -4.113559 -2.441473  0.0\n",
      "90  -5.048572 -6.890339 -6.358375 -5.112200 -3.921078  0.0\n",
      "46  -4.925511 -5.769917 -3.829237 -5.842151 -6.037171  0.0\n",
      "73  -5.371130 -8.184374 -4.563429 -6.120754 -4.264003  0.0\n",
      "93  -5.157821 -3.543431 -4.919152 -4.945978 -6.869391  0.0\n",
      "76  -5.883253 -5.779150 -4.552248 -5.179388 -5.336047  0.0\n",
      "286  6.419294  2.480184  5.198751 -4.277643 -6.110710  1.0\n",
      "60   5.221120  5.062040  4.517759 -4.005868 -5.491844  1.0\n",
      "77  -4.710271 -6.081011 -2.880744 -5.847287 -5.584336  0.0\n",
      "63  -5.229561 -5.357825 -5.491552 -9.065681 -9.396040  0.0\n",
      "234  3.680948  5.276912  5.290229 -6.505245 -3.995211  1.0\n",
      "229 -6.663323 -4.941028 -3.568090 -4.676361 -7.376549  0.0\n",
      "111 -6.545252 -4.214647 -6.361550 -3.704744 -5.087839  0.0\n",
      "231 -6.092041 -4.296750 -3.918834 -6.369763 -8.030941  0.0\n",
      "180 -2.621365 -4.486879 -4.235128 -8.038417 -5.751386  0.0\n",
      "144 -5.196398 -7.784647 -3.645233 -5.081374 -5.688016  0.0\n",
      "239  5.184361  5.052542  5.730835 -5.304008 -5.205282  1.0\n",
      "75  -5.128680 -4.853108 -4.840055 -7.099775 -7.518304  0.0\n",
      "297 -3.484287 -4.879955 -3.651828 -6.977256 -5.281805  0.0\n",
      "278 -4.601134 -6.710756 -4.655974 -5.790876 -5.086411  0.0\n",
      "97  -5.054803 -3.449882 -6.134525 -3.722335 -4.435956  0.0\n",
      "92  -4.492584 -6.307864 -5.850904 -6.343005 -5.239047  0.0\n",
      "192 -5.164568 -4.443414 -5.110140 -4.561745 -5.746426  0.0\n",
      "25  -4.398044 -4.908111 -4.918442 -4.544982 -3.615148  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into a train set = 80% and test = 20%\n",
    "data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#Print the shape of the train and test set\n",
    "print(f\"Train Data shape: \\n{data_train}\\n\")\n",
    "print(f\"Test Data shape: \\n{data_test}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_train: \n",
      "       5\n",
      "232  0.0\n",
      "59   0.0\n",
      "6    0.0\n",
      "185  1.0\n",
      "173  0.0\n",
      "..   ...\n",
      "188  0.0\n",
      "71   0.0\n",
      "106  1.0\n",
      "270  0.0\n",
      "102  0.0\n",
      "\n",
      "[240 rows x 1 columns]\n",
      "\n",
      "labels_test: \n",
      "       5\n",
      "203  0.0\n",
      "266  0.0\n",
      "152  1.0\n",
      "9    0.0\n",
      "233  1.0\n",
      "226  1.0\n",
      "196  1.0\n",
      "109  1.0\n",
      "5    0.0\n",
      "175  0.0\n",
      "237  1.0\n",
      "57   0.0\n",
      "218  0.0\n",
      "45   0.0\n",
      "182  0.0\n",
      "221  0.0\n",
      "289  0.0\n",
      "211  0.0\n",
      "148  0.0\n",
      "165  0.0\n",
      "78   0.0\n",
      "113  1.0\n",
      "249  0.0\n",
      "250  1.0\n",
      "104  0.0\n",
      "42   1.0\n",
      "281  1.0\n",
      "295  0.0\n",
      "157  0.0\n",
      "238  0.0\n",
      "17   1.0\n",
      "164  0.0\n",
      "33   0.0\n",
      "24   1.0\n",
      "215  1.0\n",
      "119  0.0\n",
      "7    0.0\n",
      "90   0.0\n",
      "46   0.0\n",
      "73   0.0\n",
      "93   0.0\n",
      "76   0.0\n",
      "286  1.0\n",
      "60   1.0\n",
      "77   0.0\n",
      "63   0.0\n",
      "234  1.0\n",
      "229  0.0\n",
      "111  0.0\n",
      "231  0.0\n",
      "180  0.0\n",
      "144  0.0\n",
      "239  1.0\n",
      "75   0.0\n",
      "297  0.0\n",
      "278  0.0\n",
      "97   0.0\n",
      "92   0.0\n",
      "192  0.0\n",
      "25   0.0\n",
      "\n",
      "features_train: \n",
      "            0         1         2         3         4\n",
      "232 -4.691545 -4.897106 -7.574577 -6.690183 -4.943022\n",
      "59  -5.540908 -3.826580 -5.718433 -5.464299 -7.812865\n",
      "6   -5.558902 -4.469969 -4.521977 -1.187704 -3.489468\n",
      "185  3.530166  6.453537  5.518635 -5.984399 -3.861131\n",
      "173 -3.874927 -4.519557 -6.019036 -5.568689 -4.379450\n",
      "..        ...       ...       ...       ...       ...\n",
      "188 -4.382753 -4.156182 -4.945608 -4.251797 -4.573749\n",
      "71  -4.917879 -5.336254 -5.289513 -8.989536 -6.432483\n",
      "106  4.376757  7.063312  6.773084 -5.074527 -4.982809\n",
      "270 -5.132905 -5.448679 -3.954188 -3.423394 -2.365563\n",
      "102 -5.199458 -4.936755 -6.149472 -5.588318 -4.449639\n",
      "\n",
      "[240 rows x 5 columns]\n",
      "\n",
      "lfeatures_test: \n",
      "            0         1         2         3         4\n",
      "203 -5.452907 -4.913225 -3.880859 -4.158012 -4.186854\n",
      "266 -5.466881 -6.877064 -6.090096 -4.461881 -4.315136\n",
      "152  4.699258  5.361881  6.027668 -4.969582 -4.990885\n",
      "9   -5.010238 -2.273145 -6.408167 -6.647583 -8.817610\n",
      "233  4.901756  4.938506  4.737213 -5.158632 -4.912881\n",
      "226  5.294774  3.697034  4.836675 -7.587754 -5.021268\n",
      "196  5.543394  3.381955  3.779498 -3.263850 -5.624578\n",
      "109  4.688117  5.969182  6.374644 -7.082709 -4.919892\n",
      "5   -5.722621 -5.869430 -5.060552 -3.792765 -3.505551\n",
      "175 -5.592691 -4.958004 -5.226683 -5.225733 -5.440950\n",
      "237  4.583797  7.546729  5.999683 -6.256342 -4.177344\n",
      "57  -4.961842 -4.873437 -5.070948 -6.117641 -5.689181\n",
      "218 -5.888881 -5.526152 -5.815783 -5.955422 -5.524494\n",
      "45  -5.487388 -3.013314 -6.009145 -3.250708 -3.947677\n",
      "182 -4.939837 -2.376040 -6.295114 -5.119049 -6.317065\n",
      "221 -6.447492 -4.088617 -6.404756 -3.276425 -5.473491\n",
      "289 -5.058468 -5.287388 -4.698604 -5.897789 -6.716347\n",
      "211 -5.574853 -4.550226 -4.400662 -5.140022 -4.908897\n",
      "148 -3.749611 -4.789767 -5.245193 -7.341026 -6.314782\n",
      "165 -5.376927 -4.819349 -3.791505 -4.480134 -4.539707\n",
      "78  -5.130252 -6.467679 -3.782191 -8.058004 -6.472565\n",
      "113  3.402719  5.003821  4.350144 -5.227083 -3.889555\n",
      "249 -4.849378 -4.553499 -4.111270 -5.061952 -4.055680\n",
      "250  5.593270  4.364744  3.769207 -5.521032 -4.824245\n",
      "104 -4.785916 -5.874768 -4.320362 -6.083843 -5.359186\n",
      "42   5.617058  5.288188  6.531158 -5.066146 -6.087117\n",
      "281  4.103299  3.925165  4.947911 -4.557117 -4.567186\n",
      "295 -4.757174 -6.271916 -5.165861 -5.624757 -4.289127\n",
      "157 -4.666674 -4.085056 -4.458655 -6.411579 -6.717503\n",
      "238 -3.508291 -3.783912 -5.245605 -5.845384 -4.929951\n",
      "17   4.574268  5.343613  4.097700 -5.448176 -4.215139\n",
      "164 -5.292489 -4.086846 -4.133240 -3.920018 -6.010868\n",
      "33  -5.117716 -5.746916 -4.387790 -4.654688 -4.485620\n",
      "24   5.083777  5.484801  5.384913 -4.399167 -5.478308\n",
      "215  6.229801  4.051011  3.094323 -5.756406 -5.123341\n",
      "119 -5.084994 -3.889805 -4.923373 -1.650257 -3.232784\n",
      "7   -3.714469 -4.916281 -4.678875 -4.113559 -2.441473\n",
      "90  -5.048572 -6.890339 -6.358375 -5.112200 -3.921078\n",
      "46  -4.925511 -5.769917 -3.829237 -5.842151 -6.037171\n",
      "73  -5.371130 -8.184374 -4.563429 -6.120754 -4.264003\n",
      "93  -5.157821 -3.543431 -4.919152 -4.945978 -6.869391\n",
      "76  -5.883253 -5.779150 -4.552248 -5.179388 -5.336047\n",
      "286  6.419294  2.480184  5.198751 -4.277643 -6.110710\n",
      "60   5.221120  5.062040  4.517759 -4.005868 -5.491844\n",
      "77  -4.710271 -6.081011 -2.880744 -5.847287 -5.584336\n",
      "63  -5.229561 -5.357825 -5.491552 -9.065681 -9.396040\n",
      "234  3.680948  5.276912  5.290229 -6.505245 -3.995211\n",
      "229 -6.663323 -4.941028 -3.568090 -4.676361 -7.376549\n",
      "111 -6.545252 -4.214647 -6.361550 -3.704744 -5.087839\n",
      "231 -6.092041 -4.296750 -3.918834 -6.369763 -8.030941\n",
      "180 -2.621365 -4.486879 -4.235128 -8.038417 -5.751386\n",
      "144 -5.196398 -7.784647 -3.645233 -5.081374 -5.688016\n",
      "239  5.184361  5.052542  5.730835 -5.304008 -5.205282\n",
      "75  -5.128680 -4.853108 -4.840055 -7.099775 -7.518304\n",
      "297 -3.484287 -4.879955 -3.651828 -6.977256 -5.281805\n",
      "278 -4.601134 -6.710756 -4.655974 -5.790876 -5.086411\n",
      "97  -5.054803 -3.449882 -6.134525 -3.722335 -4.435956\n",
      "92  -4.492584 -6.307864 -5.850904 -6.343005 -5.239047\n",
      "192 -5.164568 -4.443414 -5.110140 -4.561745 -5.746426\n",
      "25  -4.398044 -4.908111 -4.918442 -4.544982 -3.615148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting Labels\n",
    "#Get a list of all columns\n",
    "columns = data_train.columns.to_list()\n",
    "#Remove the label and save it\n",
    "columns_drop = columns.pop(-1)\n",
    "\n",
    "#Remove all labels except for the label in the train and test dataframe\n",
    "labels_train = data_train.drop(columns, axis=1)\n",
    "labels_test = data_test.drop(columns, axis=1)\n",
    "\n",
    "#Print the labesl of the test and train\n",
    "print(f\"labels_train: \\n{labels_train}\\n\")\n",
    "print(f\"labels_test: \\n{labels_test}\\n\")\n",
    "\n",
    "#Remove the label from the train and test dataframe\n",
    "features_train = data_train.drop(['5'], axis=1)\n",
    "features_test = data_test.drop(['5'], axis=1)\n",
    "\n",
    "#Print the features of the train and test dataset\n",
    "print(f\"features_train: \\n{features_train }\\n\")\n",
    "print(f\"lfeatures_test: \\n{features_test }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTrainFlat = labels_train.values.ravel()\n",
    "\n",
    "#Fit one vs rest Gradient Boosting classification\n",
    "gradientBoosting = GradientBoostingClassifier()\n",
    "gradientBoosting = gradientBoosting.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Fit RandomForestClassifier classification\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest = randomForest.fit(features_train,labelTrainFlat)\n",
    "\n",
    "#Create a KNeighbors classification object\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors = kNeighbors.fit(features_train,labelTrainFlat)\n",
    "\n",
    "#Create an LogisticRegression object\n",
    "logisticRegression = LogisticRegression(max_iter=5000)\n",
    "logisticRegression = logisticRegression.fit(features_train,labelTrainFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters GradientBoosting: \n",
      "{'learning_rate': 0.44, 'min_samples_leaf': 5, 'min_samples_split': 7, 'n_estimators': 57}\n",
      "\n",
      "Best estimator GradientBoosting: \n",
      "GradientBoostingClassifier(learning_rate=0.44, min_samples_leaf=5,\n",
      "                           min_samples_split=7, n_estimators=57)\n",
      "\n",
      "Best score GradientBoosting: \n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of GradientBoosting for GridSearchCV\n",
    "parametersGradientBoosting = [\n",
    "    {'learning_rate': [0.44,0.45,0.46],'min_samples_leaf': [5,6,7],'min_samples_split': [7,8,9,10], 'n_estimators': [57,58,59,60]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchGradientBoosting = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the GradientBoosting \n",
    "grid_searchGradientBoosting.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters GradientBoosting: \\n{grid_searchGradientBoosting.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting: \\n{grid_searchGradientBoosting.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting: \\n{grid_searchGradientBoosting.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 1, 'p': 1, 'weights': 'uniform'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=1, p=1)\n",
      "\n",
      "Best score KNeighbors: \n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of KNeighbors for GridSearchCV\n",
    "parametersKNeighbors = [\n",
    "    {'n_neighbors': [1,2,3],'weights':['uniform', 'distance'],'algorithm':['auto'], 'p': [1,2,3]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "#Preform KNeighbors to find best parameters\n",
    "grid_searchKNeighbors = GridSearchCV(kNeighbors, parametersKNeighbors, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the KNeighbors \n",
    "grid_searchKNeighbors.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters KNeighbors: \\n{grid_searchKNeighbors.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors: \\n{grid_searchKNeighbors.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors: \\n{grid_searchKNeighbors.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters Logistic Regression: \n",
      "{'C': 1, 'multi_class': 'ovr', 'penalty': 'none'}\n",
      "\n",
      "Best estimator Logistic Regression: \n",
      "LogisticRegression(C=1, max_iter=5000, multi_class='ovr', penalty='none')\n",
      "\n",
      "Best score Logistic Regression: \n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of LogisticRegression for GridSearchCV\n",
    "parametersLogisticRegression = [\n",
    "    {'multi_class': ['ovr'],'penalty':['none','l2'], 'C': [1,2,3]}\n",
    "]\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "#Preform LogisticRegression to find best parameters\n",
    "grid_searchLogisticRegression = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the LogisticRegression \n",
    "grid_searchLogisticRegression.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print LogisticRegression Results\n",
    "print(f\"Best parameters Logistic Regression: \\n{grid_searchLogisticRegression.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression: \\n{grid_searchLogisticRegression.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression: \\n{grid_searchLogisticRegression.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters RandomForest: \n",
      "{'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_split': 0.05, 'n_estimators': 145}\n",
      "\n",
      "Best estimator RandomForest: \n",
      "RandomForestClassifier(max_depth=10, max_features='auto',\n",
      "                       min_samples_split=0.05, n_estimators=145)\n",
      "\n",
      "Best score RandomForest: \n",
      "1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of RandomForest for GridSearchCV\n",
    "parametersRandomForest = [\n",
    "    {'n_estimators': [145,150,155,190],'max_depth': [10,12], 'bootstrap': [True, False],\n",
    "     'min_samples_split': [0.05,2], 'max_features': ['auto']}\n",
    "]\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchRandomForest = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the RandomForest \n",
    "grid_searchRandomForest.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters RandomForest: \\n{grid_searchRandomForest.best_params_}\\n\")\n",
    "print(f\"Best estimator RandomForest: \\n{grid_searchRandomForest.best_estimator_}\\n\")\n",
    "print(f\"Best score RandomForest: \\n{grid_searchRandomForest.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Accuracy for Gradient Boosting: \n",
      "1.0\n",
      "\n",
      "Balanced Test Accuracy for Gradient Boosting: \n",
      "1.0\n",
      "\n",
      "Mean F1 Macro for Gradient Boosting: \n",
      "1.0\n",
      "\n",
      "Mean Test Accuracy for Random Forests: \n",
      "1.0\n",
      "\n",
      "Balanced Test Accuracy for Random Forests: \n",
      "1.0\n",
      "\n",
      "Mean F1 Macro for Random Forests: \n",
      "1.0\n",
      "\n",
      "Mean Test Accuracy for Logistic Regression: \n",
      "1.0\n",
      "\n",
      "Balanced Test Accuracy for Logistic Regression: \n",
      "1.0\n",
      "\n",
      "Mean F1 Macro for Logistic Regression: \n",
      "1.0\n",
      "\n",
      "Mean Test Accuracy for K Nearest Neighbours: \n",
      "1.0\n",
      "\n",
      "Balanced Test Accuracy for K Nearest Neighbours: \n",
      "1.0\n",
      "\n",
      "Mean F1 Macro for Logistic K Nearest Neighbours: \n",
      "1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the results for all classifiers \n",
    "cross_val_resultsGB = grid_searchGradientBoosting.cv_results_\n",
    "cross_val_resultsRF = grid_searchRandomForest.cv_results_\n",
    "cross_val_resultsLR = grid_searchLogisticRegression.cv_results_\n",
    "cross_val_resultsKN = grid_searchKNeighbors.cv_results_\n",
    "\n",
    "\n",
    "#Print the results of all classiifiers\n",
    "#GBC\n",
    "print(f\"Mean Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_F1_macro'])}\\n\")\n",
    "\n",
    "#RFC\n",
    "print(f\"Mean Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_F1_macro'])}\\n\")\n",
    "\n",
    "#LRC\n",
    "print(f\"Mean Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_F1_macro'])}\\n\")\n",
    "\n",
    "#KNC\n",
    "print(f\"Mean Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Logistic K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_F1_macro'])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ea9996f2a91c7d112e626959c304b606e4bf2254e73fec145d965796b2ca69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
