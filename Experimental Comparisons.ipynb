{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, cross_validate\n",
    "\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#multiclass imports\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier #Will identify the maority calss base line, model needs to do better then the baseline\n",
    "\n",
    "from statistics import mean\n",
    "# to reduce randomness then you put the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "from ArtificialImmuneSystem import *\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\GeneratedSyntheticData-testing.csv')\n",
    "#df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: \n",
      "(300, 6)\n",
      "\n",
      "Data size: \n",
      "1800\n",
      "\n",
      "Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Old Class Distribution: Counter({0.0: 247, 1.0: 53})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Data size: \\n{df.size}\\n\")\n",
    "print(f\"Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Old Class Distribution: {Counter(df['5'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "            0         1         2         3         4    5\n",
      "232 -0.019817  0.058038 -1.455309  0.023516  0.054193  0.0\n",
      "59   0.055881 -0.163657  0.666498 -0.066311 -0.152815  0.0\n",
      "6    0.041093 -0.120351  1.869881 -0.048764 -0.112377  0.0\n",
      "185  0.148608 -0.435229 -0.695417 -0.176347 -0.406396  1.0\n",
      "173 -0.013564  0.039726  0.225626  0.016096  0.037094  1.0\n",
      "..        ...       ...       ...       ...       ...  ...\n",
      "188  0.078593 -0.230175 -0.700165 -0.093263 -0.214926  0.0\n",
      "71  -0.016761  0.049087 -1.858915  0.019889  0.045835  1.0\n",
      "106  0.047683 -0.139649 -0.473959 -0.056583 -0.130397  0.0\n",
      "270  0.147648 -0.432416 -0.864888 -0.175207 -0.403769  0.0\n",
      "102 -0.004327  0.012672  0.695998  0.005134  0.011832  0.0\n",
      "\n",
      "[240 rows x 6 columns]\n",
      "\n",
      "Test Data shape: \n",
      "            0         1         2         3         4    5\n",
      "203 -0.321374  0.941208  0.287581  0.381360  0.878854  1.0\n",
      "266  0.264000 -0.773176  0.919191 -0.313277 -0.721955  1.0\n",
      "152  0.027106 -0.079385  1.432811 -0.032165 -0.074126  0.0\n",
      "9   -0.251536  0.736675  0.503760  0.298487  0.687871  1.0\n",
      "233 -0.041580  0.121775  0.800144  0.049341  0.113708  0.0\n",
      "226 -0.041806  0.122438  0.169571  0.049609  0.114326  1.0\n",
      "196  0.039404 -0.115401  0.260380 -0.046759 -0.107756  0.0\n",
      "109  0.082851 -0.242646 -1.106080 -0.098316 -0.226571  0.0\n",
      "5    0.062787 -0.183884  0.826365 -0.074507 -0.171702  0.0\n",
      "175 -0.003825  0.011204 -1.924100  0.004539  0.010461  0.0\n",
      "237  0.083398 -0.244248  1.467195 -0.098965 -0.228067  0.0\n",
      "57   0.051158 -0.149827 -0.406991 -0.060707 -0.139901  0.0\n",
      "218 -0.482802  1.413983 -0.439613  0.572920  1.320309  1.0\n",
      "45   0.042370 -0.124090 -1.033826 -0.050279 -0.115870  0.0\n",
      "182  0.064293 -0.188295 -0.252846 -0.076294 -0.175821  0.0\n",
      "221  0.031865 -0.093322  0.691377 -0.037813 -0.087140  0.0\n",
      "289 -0.016949  0.049639  0.714885  0.020113  0.046350  0.0\n",
      "211  0.018044 -0.052847  0.948641 -0.021412 -0.049346  0.0\n",
      "148  0.102059 -0.298900  1.294859 -0.121109 -0.279098  1.0\n",
      "165  0.014979 -0.043869 -1.333941 -0.017775 -0.040963  0.0\n",
      "78   0.029495 -0.086381 -0.786875 -0.035000 -0.080658  0.0\n",
      "113  0.015796 -0.046261  0.652778 -0.018744 -0.043197  1.0\n",
      "249  0.149643 -0.438261 -0.026008 -0.177575 -0.409227  0.0\n",
      "250 -0.055383  0.162199  0.100424  0.065720  0.151454  0.0\n",
      "104  0.047499 -0.139110  0.879414 -0.056365 -0.129894  0.0\n",
      "42   0.051770 -0.151619  0.138218 -0.061433 -0.141575  0.0\n",
      "281  0.155715 -0.456042 -1.154290 -0.184780 -0.425830  0.0\n",
      "295  0.083178 -0.243602  0.357564 -0.098703 -0.227464  0.0\n",
      "157  0.055724 -0.163200 -1.133110 -0.066126 -0.152388  0.0\n",
      "238  0.126452 -0.370340 -0.064334 -0.150055 -0.345806  0.0\n",
      "17   0.118478 -0.346988 -0.139005 -0.140593 -0.324001  0.0\n",
      "164  0.139004 -0.407100  1.283321 -0.164949 -0.380130  0.0\n",
      "33   0.056930 -0.166730 -0.928395 -0.067556 -0.155685  0.0\n",
      "24   0.097531 -0.285638 -0.093401 -0.115735 -0.266715  1.0\n",
      "215  0.038883 -0.113877  0.670765 -0.046141 -0.106333  0.0\n",
      "119 -0.259853  0.761033  2.332351  0.308357  0.710616  1.0\n",
      "7    0.276915 -0.811002  0.296344 -0.328603 -0.757274  1.0\n",
      "90   0.040208 -0.117756  0.651974 -0.047713 -0.109955  0.0\n",
      "46  -0.013564  0.039725 -0.537653  0.016096  0.037093  0.0\n",
      "73   0.043639 -0.127807 -0.086592 -0.051785 -0.119340  0.0\n",
      "93   0.068588 -0.200875 -0.513328 -0.081391 -0.187567  0.0\n",
      "76   0.107918 -0.316060  1.713443 -0.128062 -0.295122  0.0\n",
      "286  0.010558 -0.030922  0.149024 -0.012529 -0.028873  0.0\n",
      "60   0.062228 -0.182247  0.996821 -0.073843 -0.170173  0.0\n",
      "77   0.085434 -0.250212 -1.137772 -0.101381 -0.233636  0.0\n",
      "63  -0.048642  0.142458  0.976490  0.057721  0.133021  0.0\n",
      "234 -0.034605  0.101346  0.172010  0.041064  0.094632  0.0\n",
      "229  0.150320 -0.440243 -0.254723 -0.178379 -0.411078  0.0\n",
      "111  0.036719 -0.107539  0.529002 -0.043573 -0.100415  0.0\n",
      "231  0.037014 -0.108403 -1.029966 -0.043923 -0.101222  0.0\n",
      "180  0.151902 -0.444875  2.230666 -0.180255 -0.415403  0.0\n",
      "144  0.048781 -0.142866 -0.849575 -0.057887 -0.133401  0.0\n",
      "239 -0.037728  0.110494  1.300327  0.044770  0.103174  0.0\n",
      "75   0.042747 -0.125194  0.523264 -0.050726 -0.116900  0.0\n",
      "297  0.146055 -0.427751 -1.035479 -0.173317 -0.399413  0.0\n",
      "278  0.120725 -0.353567 -0.815155 -0.143259 -0.330144  0.0\n",
      "97   0.036656 -0.107353  1.624771 -0.043498 -0.100241  0.0\n",
      "92   0.048539 -0.142156 -1.564186 -0.057599 -0.132739  0.0\n",
      "192  0.055258 -0.161834  0.921478 -0.065572 -0.151113  0.0\n",
      "25   0.034777 -0.101853  0.407796 -0.041269 -0.095105  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into a train set = 80% and test = 20%\n",
    "data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#Print the shape of the train and test set\n",
    "print(f\"Train Data shape: \\n{data_train}\\n\")\n",
    "print(f\"Test Data shape: \\n{data_test}\\n\")\n",
    "\n",
    "data_train_AIS = data_train.copy()\n",
    "data_train_SMOTE = data_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_feat_train before:  (192, 5)\n",
      "origin_labels_train before:  Counter({0.0: 156, 1.0: 36})\n",
      "origin_feat_train after:  (240, 5)\n",
      "population_features:  (154, 5)\n",
      "origin_labels_train after:  Counter({0.0: 194, 1.0: 46})\n",
      "origin_feat_train before:  (192, 5)\n",
      "origin_labels_train before:  Counter({0.0: 155, 1.0: 37})\n",
      "origin_feat_train after:  (240, 5)\n",
      "population_features:  (154, 5)\n",
      "origin_labels_train after:  Counter({0.0: 191, 1.0: 49})\n",
      "score1: 0.6795085470085469\n",
      "score2: 0.5965743686138423\n",
      "origin_feat_train before:  (192, 5)\n",
      "origin_labels_train before:  Counter({0.0: 159, 1.0: 33})\n",
      "origin_feat_train after:  (240, 5)\n",
      "population_features:  (154, 5)\n",
      "origin_labels_train after:  Counter({0.0: 199, 1.0: 41})\n",
      "score1: 0.6795085470085469\n",
      "score2: 0.5618727106227106\n",
      "origin_feat_train before:  (192, 5)\n",
      "origin_labels_train before:  Counter({0.0: 158, 1.0: 34})\n",
      "origin_feat_train after:  (240, 5)\n",
      "population_features:  (154, 5)\n",
      "origin_labels_train after:  Counter({0.0: 199, 1.0: 41})\n",
      "score1: 0.6795085470085469\n",
      "score2: 0.6359093765711413\n",
      "origin_feat_train before:  (192, 5)\n",
      "origin_labels_train before:  Counter({0.0: 155, 1.0: 37})\n",
      "origin_feat_train after:  (240, 5)\n",
      "population_features:  (154, 5)\n",
      "origin_labels_train after:  Counter({0.0: 191, 1.0: 49})\n",
      "score1: 0.6795085470085469\n",
      "score2: 0.5480899582873267\n",
      "origin_feat_train before:  (192, 5)\n",
      "origin_labels_train before:  Counter({0.0: 161, 1.0: 31})\n",
      "origin_feat_train after:  (240, 5)\n",
      "population_features:  (154, 5)\n",
      "origin_labels_train after:  Counter({0.0: 205, 1.0: 35})\n",
      "score1: 0.6795085470085469\n",
      "score2: 0.6334634257484064\n",
      "SMOTE Oversampled Data shape: \n",
      "(394, 6)\n",
      "\n",
      "SMOTE Oversampled Data size: \n",
      "2364\n",
      "\n",
      "SMOTE Oversampled Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "AIS Oversampled Data shape: \n",
      "(394, 6)\n",
      "\n",
      "AIS Oversampled Data size: \n",
      "2364\n",
      "\n",
      "AIS Oversampled Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "New SMOTE Class Distribution: Counter({0.0: 197, 1.0: 197})\n",
      "New AIS Class Distribution: Counter({0.0: 197, 1.0: 197})\n",
      "Old Class Distribution: Counter({0.0: 197, 1.0: 43})\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create an oversampling object\n",
    "oversample = SMOTE()\n",
    "oversample_AIS = ArtificialImmuneSystem()\n",
    "#Oversample and add to the dataframe to fix the class imbalance\n",
    "randomForest = RandomForestClassifier()\n",
    "x_over, y_over = oversample.fit_resample(data_train_SMOTE.drop([\"5\"], axis=1), data_train_SMOTE.drop(data_train_SMOTE.columns[0:-1],axis=1))\n",
    "input_x_over_AIS, y_over_AIS = oversample_AIS.AIS_Resample(data_train_AIS.drop([\"5\"], axis=1), data_train_AIS.drop(data_train_AIS.columns[0:-1],axis=1), 20, 5, randomForest,5,'balanced_accuracy' )\n",
    "\n",
    "smote_df = pd.concat([x_over, y_over], axis=1)\n",
    "ais_df = pd.concat([input_x_over_AIS, y_over_AIS], axis=1)\n",
    "\n",
    "# print the dimensionality of the oversampled dataset\n",
    "print(f\"SMOTE Oversampled Data shape: \\n{smote_df.shape}\\n\")\n",
    "print(f\"SMOTE Oversampled Data size: \\n{smote_df.size}\\n\")\n",
    "print(f\"SMOTE Oversampled Data ndim: \\n{smote_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the dimensionality of the oversampled dataset\n",
    "print(f\"AIS Oversampled Data shape: \\n{ais_df.shape}\\n\")\n",
    "print(f\"AIS Oversampled Data size: \\n{ais_df.size}\\n\")\n",
    "print(f\"AIS Oversampled Data ndim: \\n{ais_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"New SMOTE Class Distribution: {Counter(smote_df['5'])}\")\n",
    "print(f\"New AIS Class Distribution: {Counter(ais_df['5'])}\")\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"Old Class Distribution: {Counter(data_train['5'])}\")\n",
    "\n",
    "print(\"_____________________________________________\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aisOversample = ArtificialImmuneSystem()\n",
    "#minority_class = df[df['5'] == 1]\n",
    "#majority_class = df[df['5'] == 0]\n",
    "\n",
    "#requiredPopulation = len(majority_class)-len(minority_class)\n",
    "#population = aisOversample.AIS(minority_class, max_rounds=100, totalPopulation=requiredPopulation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Labels\n",
    "#Get a list of all columns\n",
    "#columns = data_train.columns.to_list()\n",
    "#Remove the label and save it\n",
    "#columns_drop = columns.pop(-1)\n",
    "\n",
    "#Remove all labels except for the label in the train and test dataframe\n",
    "#labels_train = data_train.drop(columns, axis=1)\n",
    "#labels_test = data_test.drop(columns, axis=1)\n",
    "\n",
    "#Print the labesl of the test and train\n",
    "#print(f\"labels_train: \\n{labels_train}\\n\")\n",
    "#print(f\"labels_test: \\n{labels_test}\\n\")\n",
    "\n",
    "#Remove the label from the train and test dataframe\n",
    "#features_train = data_train.drop(['5'], axis=1)\n",
    "#features_test = data_test.drop(['5'], axis=1)\n",
    "\n",
    "#Print the features of the train and test dataset\n",
    "#print(f\"features_train: \\n{features_train }\\n\")\n",
    "#print(f\"lfeatures_test: \\n{features_test }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelTrainFlat = labels_train.values.ravel()\n",
    "\n",
    "#Fit one vs rest Gradient Boosting classification\n",
    "gradientBoosting = GradientBoostingClassifier()\n",
    "gradientBoosting = gradientBoosting.fit(x_over, y_over.values.ravel())\n",
    "\n",
    "gradientBoosting_AIS = GradientBoostingClassifier()\n",
    "gradientBoosting_AIS = gradientBoosting.fit(input_x_over_AIS, y_over_AIS.values.ravel())\n",
    "\n",
    "#Fit RandomForestClassifier classification\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest = randomForest.fit(x_over,y_over.values.ravel())\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest_AIS  = randomForest.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Create a KNeighbors classification object\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors = kNeighbors.fit(x_over,y_over.values.ravel())\n",
    "\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors_AIS  = kNeighbors.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Create an LogisticRegression object\n",
    "logisticRegression = LogisticRegression(max_iter=5000)\n",
    "logisticRegression = logisticRegression.fit(x_over,y_over.values.ravel())\n",
    "\n",
    "logisticRegression = LogisticRegression(max_iter=5000)\n",
    "logisticRegression_AIS  = logisticRegression.fit(input_x_over_AIS,y_over_AIS.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m grid_searchGradientBoosting_AIS \u001b[39m=\u001b[39m GridSearchCV(gradientBoosting, parametersGradientBoosting, cv\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, scoring \u001b[39m=\u001b[39m scoringX, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, refit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbal_accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39m#Fit the GradientBoosting \u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m grid_searchGradientBoosting\u001b[39m.\u001b[39mfit(x_over, y_over\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel())\n\u001b[0;32m     15\u001b[0m grid_searchGradientBoosting_AIS\u001b[39m.\u001b[39mfit(input_x_over_AIS,y_over_AIS\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel())\n\u001b[0;32m     17\u001b[0m \u001b[39m#Print GridSearchCV Results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\concurrent\\futures\\_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 440\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Set the parameters of GradientBoosting for GridSearchCV\n",
    "parametersGradientBoosting = [\n",
    "    {'learning_rate': [0.44,0.45,0.46],'min_samples_leaf': [5,6,7],'min_samples_split': [7,8,9,10], 'n_estimators': [57,58,59,60]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"roc_auc\": \"roc_auc\", \"bal_accuracy\": \"balanced_accuracy\"}\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchGradientBoosting = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchGradientBoosting_AIS = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the GradientBoosting \n",
    "grid_searchGradientBoosting.fit(x_over, y_over.values.ravel())\n",
    "grid_searchGradientBoosting_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters GradientBoosting: \\n{grid_searchGradientBoosting.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting: \\n{grid_searchGradientBoosting.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting: \\n{grid_searchGradientBoosting.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters GradientBoosting AIS: \\n{grid_searchGradientBoosting_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting AIS: \\n{grid_searchGradientBoosting_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting AIS: \\n{grid_searchGradientBoosting_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
      "\n",
      "Best score KNeighbors: \n",
      "0.7614285714285715\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 2, 'p': 3, 'weights': 'uniform'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=2, p=3)\n",
      "\n",
      "Best score KNeighbors: \n",
      "0.865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of KNeighbors for GridSearchCV\n",
    "parametersKNeighbors = [\n",
    "    {'n_neighbors': [1,2,3],'weights':['uniform', 'distance'],'algorithm':['auto'], 'p': [1,2,3]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"roc_auc\": \"roc_auc\", \"bal_accuracy\": \"balanced_accuracy\"}\n",
    "\n",
    "#Preform KNeighbors to find best parameters\n",
    "grid_searchKNeighbors = GridSearchCV(kNeighbors, parametersKNeighbors, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchKNeighbors_AIS = GridSearchCV(kNeighbors, parametersKNeighbors, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the KNeighbors \n",
    "grid_searchKNeighbors.fit(x_over, y_over.values.ravel())\n",
    "grid_searchKNeighbors_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters KNeighbors: \\n{grid_searchKNeighbors.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors: \\n{grid_searchKNeighbors.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors: \\n{grid_searchKNeighbors.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters KNeighbors AIS: \\n{grid_searchKNeighbors_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors AIS: \\n{grid_searchKNeighbors_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors AIS: \\n{grid_searchKNeighbors_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters Logistic Regression: \n",
      "{'C': 1, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\n",
      "Best estimator Logistic Regression: \n",
      "LogisticRegression(C=1, max_iter=5000, multi_class='ovr')\n",
      "\n",
      "Best score Logistic Regression: \n",
      "0.5760204081632654\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Best parameters Logistic Regression AIS: \n",
      "{'C': 1, 'multi_class': 'ovr', 'penalty': 'none'}\n",
      "\n",
      "Best estimator Logistic Regression AIS: \n",
      "LogisticRegression(C=1, max_iter=5000, multi_class='ovr', penalty='none')\n",
      "\n",
      "Best score Logistic Regression AIS: \n",
      "0.7356632653061225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of LogisticRegression for GridSearchCV\n",
    "parametersLogisticRegression = [\n",
    "    {'multi_class': ['ovr'],'penalty':['none','l2'], 'C': [1,2,3]}\n",
    "]\n",
    "scoringX = {\"roc_auc\": \"roc_auc\", \"bal_accuracy\": \"balanced_accuracy\"}\n",
    "\n",
    "#Preform LogisticRegression to find best parameters\n",
    "grid_searchLogisticRegression = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchLogisticRegression_AIS = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the LogisticRegression \n",
    "grid_searchLogisticRegression.fit(x_over, y_over.values.ravel())\n",
    "grid_searchLogisticRegression_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print LogisticRegression Results\n",
    "print(f\"Best parameters Logistic Regression: \\n{grid_searchLogisticRegression.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression: \\n{grid_searchLogisticRegression.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression: \\n{grid_searchLogisticRegression.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters Logistic Regression AIS: \\n{grid_searchLogisticRegression_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression AIS: \\n{grid_searchLogisticRegression_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression AIS: \\n{grid_searchLogisticRegression_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters RandomForest: \n",
      "{'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_split': 0.05, 'n_estimators': 190}\n",
      "\n",
      "Best estimator RandomForest: \n",
      "RandomForestClassifier(bootstrap=False, max_depth=10, min_samples_split=0.05,\n",
      "                       n_estimators=190)\n",
      "\n",
      "Best score RandomForest: \n",
      "0.746326530612245\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Best parameters RandomForest AIS: \n",
      "{'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_split': 0.05, 'n_estimators': 150}\n",
      "\n",
      "Best estimator RandomForest AIS: \n",
      "RandomForestClassifier(max_depth=10, min_samples_split=0.05, n_estimators=150)\n",
      "\n",
      "Best score RandomForest AIS: \n",
      "0.8981122448979592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of RandomForest for GridSearchCV\n",
    "parametersRandomForest = [\n",
    "    {'n_estimators': [145,150,155,190],'max_depth': [10,12], 'bootstrap': [True, False],\n",
    "     'min_samples_split': [0.05,2], 'max_features': ['auto']}\n",
    "]\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchRandomForest = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchRandomForest_AIS = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the RandomForest \n",
    "grid_searchRandomForest.fit(x_over, y_over.values.ravel())\n",
    "grid_searchRandomForest_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters RandomForest: \\n{grid_searchRandomForest.best_params_}\\n\")\n",
    "print(f\"Best estimator RandomForest: \\n{grid_searchRandomForest.best_estimator_}\\n\")\n",
    "print(f\"Best score RandomForest: \\n{grid_searchRandomForest.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters RandomForest AIS: \\n{grid_searchRandomForest_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator RandomForest AIS: \\n{grid_searchRandomForest_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score RandomForest AIS: \\n{grid_searchRandomForest_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Accuracy for Gradient Boosting: \n",
      "0.8266026814637419\n",
      "\n",
      "Balanced Test Accuracy for Gradient Boosting: \n",
      "0.7436337868480726\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Gradient Boosting AIS: \n",
      "0.9116964141098616\n",
      "\n",
      "Balanced Test Accuracy for Gradient Boosting AIS: \n",
      "0.8522293083900226\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Random Forests: \n",
      "0.7946871095376926\n",
      "\n",
      "Balanced Test Accuracy for Random Forests: \n",
      "0.7229607780612245\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Random Forests AIS: \n",
      "0.9183122917534361\n",
      "\n",
      "Balanced Test Accuracy for Random Forests AIS: \n",
      "0.8727551020408163\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Logistic Regression: \n",
      "0.5824836873524919\n",
      "\n",
      "Balanced Test Accuracy for Logistic Regression: \n",
      "0.5726190476190476\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Logistic Regression AIS: \n",
      "0.6732719005969734\n",
      "\n",
      "Balanced Test Accuracy for Logistic Regression AIS: \n",
      "0.734812925170068\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for K Nearest Neighbours: \n",
      "0.7880133856263594\n",
      "\n",
      "Balanced Test Accuracy for K Nearest Neighbours: \n",
      "0.7410345804988662\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for K Nearest Neighbours AIS: \n",
      "0.8565602295339905\n",
      "\n",
      "Balanced Test Accuracy for K Nearest Neighbours AIS: \n",
      "0.8272335600907029\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the results for all classifiers \n",
    "cross_val_resultsGB = grid_searchGradientBoosting.cv_results_\n",
    "cross_val_resultsRF = grid_searchRandomForest.cv_results_\n",
    "cross_val_resultsLR = grid_searchLogisticRegression.cv_results_\n",
    "cross_val_resultsKN = grid_searchKNeighbors.cv_results_\n",
    "\n",
    "cross_val_resultsGB_AIS = grid_searchGradientBoosting_AIS.cv_results_\n",
    "cross_val_resultsRF_AIS = grid_searchRandomForest_AIS.cv_results_\n",
    "cross_val_resultsLR_AIS = grid_searchLogisticRegression_AIS.cv_results_\n",
    "cross_val_resultsKN_AIS = grid_searchKNeighbors_AIS.cv_results_\n",
    "\n",
    "\n",
    "#Print the results of all classiifiers\n",
    "#GBC\n",
    "print(f\"Mean Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for Gradient Boosting AIS: \\n{mean(cross_val_resultsGB_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Gradient Boosting AIS: \\n{mean(cross_val_resultsGB_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")\n",
    "#RFC\n",
    "print(f\"Mean Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for Random Forests AIS: \\n{mean(cross_val_resultsRF_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests AIS: \\n{mean(cross_val_resultsRF_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")\n",
    "#LRC\n",
    "print(f\"Mean Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for Logistic Regression AIS: \\n{mean(cross_val_resultsLR_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Logistic Regression AIS: \\n{mean(cross_val_resultsLR_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")\n",
    "\n",
    "#KNC\n",
    "print(f\"Mean Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for K Nearest Neighbours AIS: \\n{mean(cross_val_resultsKN_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for K Nearest Neighbours AIS: \\n{mean(cross_val_resultsKN_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "True Positive(TP)  =  6\n",
      "False Positive(FP) =  11\n",
      "True Negative(TN)  =  39\n",
      "False Negative(FN) =  4\n",
      "__________________________________\n",
      "Random Forest\n",
      "True Positive(TP)  =  8\n",
      "False Positive(FP) =  9\n",
      "True Negative(TN)  =  41\n",
      "False Negative(FN) =  2\n",
      "__________________________________\n",
      "Linear Regresion\n",
      "True Positive(TP)  =  6\n",
      "False Positive(FP) =  22\n",
      "True Negative(TN)  =  28\n",
      "False Negative(FN) =  4\n",
      "__________________________________\n",
      "K Neighbors\n",
      "True Positive(TP)  =  4\n",
      "False Positive(FP) =  9\n",
      "True Negative(TN)  =  41\n",
      "False Negative(FN) =  6\n",
      "__________________________________\n",
      "Gradient Boosting AIS\n",
      "True Positive(TP)  =  8\n",
      "False Positive(FP) =  3\n",
      "True Negative(TN)  =  47\n",
      "False Negative(FN) =  2\n",
      "__________________________________\n",
      "Random Forest AIS\n",
      "True Positive(TP)  =  6\n",
      "False Positive(FP) =  0\n",
      "True Negative(TN)  =  50\n",
      "False Negative(FN) =  4\n",
      "__________________________________\n",
      "Linear Regresion AIS\n",
      "True Positive(TP)  =  5\n",
      "False Positive(FP) =  6\n",
      "True Negative(TN)  =  44\n",
      "False Negative(FN) =  5\n",
      "__________________________________\n",
      "K Neighbors AIS\n",
      "True Positive(TP)  =  2\n",
      "False Positive(FP) =  0\n",
      "True Negative(TN)  =  50\n",
      "False Negative(FN) =  8\n",
      "__________________________________\n",
      "Geometric Mean Score for Gradient Boosting: \n",
      "0.6841052550594827\n",
      "\n",
      "Geometric Mean Score for Random Forest: \n",
      "0.8099382692526635\n",
      "\n",
      "Geometric Mean Score for Logestic Regression: \n",
      "0.5796550698475775\n",
      "\n",
      "Geometric Mean Score for K Neighbors: \n",
      "0.5727128425310541\n",
      "\n",
      "Geometric Mean Score for Gradient Boosting AIS: \n",
      "0.867179335547152\n",
      "\n",
      "Geometric Mean Score for Random Forest AIS: \n",
      "0.7745966692414834\n",
      "\n",
      "Geometric Mean Score for Logestic Regression AIS: \n",
      "0.6633249580710799\n",
      "\n",
      "Geometric Mean Score for K Neighbors AIS: \n",
      "0.4472135954999579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test_over_GB = grid_searchGradientBoosting.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_RF = grid_searchRandomForest.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_LR = grid_searchLogisticRegression.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_KN = grid_searchKNeighbors.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "\n",
    "predictions_test_over_GB_AIS = grid_searchGradientBoosting_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_RF_AIS = grid_searchRandomForest_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_LR_AIS = grid_searchLogisticRegression_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_KN_AIS = grid_searchKNeighbors_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB).ravel()\n",
    "print(\"Gradient Boosting\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF).ravel()\n",
    "print(\"Random Forest\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR).ravel()\n",
    "print(\"Linear Regresion\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN).ravel()\n",
    "print(\"K Neighbors\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB_AIS).ravel()\n",
    "print(\"Gradient Boosting AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF_AIS).ravel()\n",
    "print(\"Random Forest AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR_AIS).ravel()\n",
    "print(\"Linear Regresion AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN_AIS).ravel()\n",
    "print(\"K Neighbors AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "\n",
    "print(f\"Geometric Mean Score for Gradient Boosting: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Random Forest: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Logestic Regression: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for K Neighbors: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "\n",
    "print(f\"Geometric Mean Score for Gradient Boosting AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB_AIS, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Random Forest AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF_AIS, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Logestic Regression AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR_AIS, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for K Neighbors AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN_AIS, labels=None, pos_label=1, average='binary',)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
