{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, cross_validate\n",
    "\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#multiclass imports\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier #Will identify the maority calss base line, model needs to do better then the baseline\n",
    "\n",
    "from statistics import mean\n",
    "# to reduce randomness then you put the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "from ArtificialImmuneSystem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\GeneratedSyntheticData-testing.csv')\n",
    "#df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: \n",
      "(300, 6)\n",
      "\n",
      "Data size: \n",
      "1800\n",
      "\n",
      "Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Old Class Distribution: Counter({0.0: 247, 1.0: 53})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Data size: \\n{df.size}\\n\")\n",
    "print(f\"Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Old Class Distribution: {Counter(df['5'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jacob\\Documents\\GitHub\\Artificial-Immune-System-For-Class-Imbalance\\Experimental Comparisons.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jacob/Documents/GitHub/Artificial-Immune-System-For-Class-Imbalance/Experimental%20Comparisons.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m randomForest \u001b[39m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jacob/Documents/GitHub/Artificial-Immune-System-For-Class-Imbalance/Experimental%20Comparisons.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x_over, y_over \u001b[39m=\u001b[39m oversample\u001b[39m.\u001b[39mfit_resample(df\u001b[39m.\u001b[39mdrop([\u001b[39m\"\u001b[39m\u001b[39m5\u001b[39m\u001b[39m\"\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), df\u001b[39m.\u001b[39mdrop(df\u001b[39m.\u001b[39mcolumns[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jacob/Documents/GitHub/Artificial-Immune-System-For-Class-Imbalance/Experimental%20Comparisons.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m input_x_over_AIS, y_over_AIS \u001b[39m=\u001b[39m oversample_AIS\u001b[39m.\u001b[39;49mAIS_Resample(df\u001b[39m.\u001b[39;49mdrop([\u001b[39m\"\u001b[39;49m\u001b[39m5\u001b[39;49m\u001b[39m\"\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), df\u001b[39m.\u001b[39;49mdrop(df\u001b[39m.\u001b[39;49mcolumns[\u001b[39m0\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), \u001b[39m20\u001b[39;49m, \u001b[39m5\u001b[39;49m, randomForest,\u001b[39m5\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mf1_macro\u001b[39;49m\u001b[39m'\u001b[39;49m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jacob/Documents/GitHub/Artificial-Immune-System-For-Class-Imbalance/Experimental%20Comparisons.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m smote_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([x_over, y_over], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jacob/Documents/GitHub/Artificial-Immune-System-For-Class-Imbalance/Experimental%20Comparisons.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# print the dimensionality of the oversampled dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Documents\\GitHub\\Artificial-Immune-System-For-Class-Imbalance\\ArtificialImmuneSystem.py:266\u001b[0m, in \u001b[0;36mArtificialImmuneSystem.AIS_Resample\u001b[1;34m(self, preparedDF, labels, max_rounds, stopping_cond, model, K_folds, scorer)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m#The number of elements we want to add to the minority class\u001b[39;00m\n\u001b[0;32m    264\u001b[0m requiredPopulation \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(overallPopulation) \u001b[39m-\u001b[39m (\u001b[39mlen\u001b[39m(minorityDF)\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 266\u001b[0m oversamples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mAIS(minorityDF,overallPopulation,max_rounds,stopping_cond,requiredPopulation,binaryColumns,model,K_folds,scorer)\n\u001b[0;32m    267\u001b[0m concatDF \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([overallPopulation,oversamples],ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseparate_df(concatDF))\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Documents\\GitHub\\Artificial-Immune-System-For-Class-Imbalance\\ArtificialImmuneSystem.py:235\u001b[0m, in \u001b[0;36mArtificialImmuneSystem.AIS\u001b[1;34m(self, minorityDF, df, max_rounds, stopping_cond, totalPopulation, binaryColumns, model, K_folds, scorer)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mwhile\u001b[39;00m( (count \u001b[39m<\u001b[39m max_rounds) \u001b[39mand\u001b[39;00m (no_change \u001b[39m<\u001b[39m stopping_cond) ):\n\u001b[0;32m    233\u001b[0m     count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcomparePopulations(current_gen,next_gen,current_labels,next_labels,model, K_folds, scorer)):\n\u001b[0;32m    237\u001b[0m         no_change \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    239\u001b[0m         current_population \u001b[39m=\u001b[39m antibody_population\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Documents\\GitHub\\Artificial-Immune-System-For-Class-Imbalance\\ArtificialImmuneSystem.py:180\u001b[0m, in \u001b[0;36mArtificialImmuneSystem.comparePopulations\u001b[1;34m(self, population1, population2, labels1, labels2, estimator, iterations, scorer)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomparePopulations\u001b[39m(\u001b[39mself\u001b[39m,population1, population2, labels1, labels2, estimator, iterations, scorer):\n\u001b[1;32m--> 180\u001b[0m     score1 \u001b[39m=\u001b[39m fmean(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitness(estimator, population1, labels1\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mravel(), iterations, scorer))\n\u001b[0;32m    181\u001b[0m     score2 \u001b[39m=\u001b[39m fmean(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfitness(estimator, population2, labels2\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mravel(), iterations, scorer))\n\u001b[0;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(score1 \u001b[39m-\u001b[39m score2) \u001b[39m<\u001b[39m \u001b[39m0.005\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Documents\\GitHub\\Artificial-Immune-System-For-Class-Imbalance\\ArtificialImmuneSystem.py:120\u001b[0m, in \u001b[0;36mArtificialImmuneSystem.fitness\u001b[1;34m(self, model, feat, label, iterations, scorer)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitness\u001b[39m(\u001b[39mself\u001b[39m, model, feat, label, iterations, scorer):\n\u001b[0;32m    118\u001b[0m     \u001b[39m#scorer is the name of the function wee aree using to evaluate our dataset\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[39m#it should be a function with signature scorer(model, feature, label) which should return only a single value.\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m cross_val_score(model, feat, label, cv \u001b[39m=\u001b[39;49m iterations, scoring \u001b[39m=\u001b[39;49m scorer)\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:254\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[39mRead more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39m[0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m--> 254\u001b[0m cv \u001b[39m=\u001b[39m check_cv(cv, y, classifier\u001b[39m=\u001b[39;49mis_classifier(estimator))\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m callable(scoring):\n\u001b[0;32m    257\u001b[0m     scorers \u001b[39m=\u001b[39m scoring\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2331\u001b[0m, in \u001b[0;36mcheck_cv\u001b[1;34m(cv, y, classifier)\u001b[0m\n\u001b[0;32m   2326\u001b[0m cv \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m \u001b[39mif\u001b[39;00m cv \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m cv\n\u001b[0;32m   2327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cv, numbers\u001b[39m.\u001b[39mIntegral):\n\u001b[0;32m   2328\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2329\u001b[0m         classifier\n\u001b[0;32m   2330\u001b[0m         \u001b[39mand\u001b[39;00m (y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2331\u001b[0m         \u001b[39mand\u001b[39;00m (type_of_target(y, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   2332\u001b[0m     ):\n\u001b[0;32m   2333\u001b[0m         \u001b[39mreturn\u001b[39;00m StratifiedKFold(cv)\n\u001b[0;32m   2334\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\utils\\multiclass.py:332\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39m# check float and contains non-integer float values\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39many(y \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)):\n\u001b[0;32m    331\u001b[0m     \u001b[39m# [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     _assert_all_finite(y, input_name\u001b[39m=\u001b[39;49minput_name)\n\u001b[0;32m    333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcontinuous\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m suffix\n\u001b[0;32m    335\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y)) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m) \u001b[39mor\u001b[39;00m (y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(y[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "#Create an oversampling object\n",
    "oversample = SMOTE()\n",
    "oversample_AIS = ArtificialImmuneSystem()\n",
    "#Oversample and add to the dataframe to fix the class imbalance\n",
    "randomForest = RandomForestClassifier()\n",
    "x_over, y_over = oversample.fit_resample(df.drop([\"5\"], axis=1), df.drop(df.columns[0:-1],axis=1))\n",
    "input_x_over_AIS, y_over_AIS = oversample_AIS.AIS_Resample(df.drop([\"5\"], axis=1), df.drop(df.columns[0:-1],axis=1), 20, 5, randomForest,5,'f1_macro' )\n",
    "smote_df = pd.concat([x_over, y_over], axis=1)\n",
    "\n",
    "\n",
    "# print the dimensionality of the oversampled dataset\n",
    "print(f\"Oversampled Data shape: \\n{smote_df.shape}\\n\")\n",
    "print(f\"Oversampled Data size: \\n{smote_df.size}\\n\")\n",
    "print(f\"Oversampled Data ndim: \\n{smote_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"New Class Distribution: {Counter(smote_df['5'])}\")\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"Old Class Distribution: {Counter(df['5'])}\")\n",
    "\n",
    "print(\"_____________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aisOversample = ArtificialImmuneSystem()\n",
    "#minority_class = df[df['5'] == 1]\n",
    "#majority_class = df[df['5'] == 0]\n",
    "\n",
    "#requiredPopulation = len(majority_class)-len(minority_class)\n",
    "#population = aisOversample.AIS(minority_class, max_rounds=100, totalPopulation=requiredPopulation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "            0         1         2         3         4    5\n",
      "232 -0.019817  0.058038 -1.455309  0.023516  0.054193  0.0\n",
      "59   0.055881 -0.163657  0.666498 -0.066311 -0.152815  0.0\n",
      "6    0.041093 -0.120351  1.869881 -0.048764 -0.112377  0.0\n",
      "185  0.148608 -0.435229 -0.695417 -0.176347 -0.406396  1.0\n",
      "173 -0.013564  0.039726  0.225626  0.016096  0.037094  1.0\n",
      "..        ...       ...       ...       ...       ...  ...\n",
      "188  0.078593 -0.230175 -0.700165 -0.093263 -0.214926  0.0\n",
      "71  -0.016761  0.049087 -1.858915  0.019889  0.045835  1.0\n",
      "106  0.047683 -0.139649 -0.473959 -0.056583 -0.130397  0.0\n",
      "270  0.147648 -0.432416 -0.864888 -0.175207 -0.403769  0.0\n",
      "102 -0.004327  0.012672  0.695998  0.005134  0.011832  0.0\n",
      "\n",
      "[240 rows x 6 columns]\n",
      "\n",
      "Test Data shape: \n",
      "            0         1         2         3         4    5\n",
      "203 -0.321374  0.941208  0.287581  0.381360  0.878854  1.0\n",
      "266  0.264000 -0.773176  0.919191 -0.313277 -0.721955  1.0\n",
      "152  0.027106 -0.079385  1.432811 -0.032165 -0.074126  0.0\n",
      "9   -0.251536  0.736675  0.503760  0.298487  0.687871  1.0\n",
      "233 -0.041580  0.121775  0.800144  0.049341  0.113708  0.0\n",
      "226 -0.041806  0.122438  0.169571  0.049609  0.114326  1.0\n",
      "196  0.039404 -0.115401  0.260380 -0.046759 -0.107756  0.0\n",
      "109  0.082851 -0.242646 -1.106080 -0.098316 -0.226571  0.0\n",
      "5    0.062787 -0.183884  0.826365 -0.074507 -0.171702  0.0\n",
      "175 -0.003825  0.011204 -1.924100  0.004539  0.010461  0.0\n",
      "237  0.083398 -0.244248  1.467195 -0.098965 -0.228067  0.0\n",
      "57   0.051158 -0.149827 -0.406991 -0.060707 -0.139901  0.0\n",
      "218 -0.482802  1.413983 -0.439613  0.572920  1.320309  1.0\n",
      "45   0.042370 -0.124090 -1.033826 -0.050279 -0.115870  0.0\n",
      "182  0.064293 -0.188295 -0.252846 -0.076294 -0.175821  0.0\n",
      "221  0.031865 -0.093322  0.691377 -0.037813 -0.087140  0.0\n",
      "289 -0.016949  0.049639  0.714885  0.020113  0.046350  0.0\n",
      "211  0.018044 -0.052847  0.948641 -0.021412 -0.049346  0.0\n",
      "148  0.102059 -0.298900  1.294859 -0.121109 -0.279098  1.0\n",
      "165  0.014979 -0.043869 -1.333941 -0.017775 -0.040963  0.0\n",
      "78   0.029495 -0.086381 -0.786875 -0.035000 -0.080658  0.0\n",
      "113  0.015796 -0.046261  0.652778 -0.018744 -0.043197  1.0\n",
      "249  0.149643 -0.438261 -0.026008 -0.177575 -0.409227  0.0\n",
      "250 -0.055383  0.162199  0.100424  0.065720  0.151454  0.0\n",
      "104  0.047499 -0.139110  0.879414 -0.056365 -0.129894  0.0\n",
      "42   0.051770 -0.151619  0.138218 -0.061433 -0.141575  0.0\n",
      "281  0.155715 -0.456042 -1.154290 -0.184780 -0.425830  0.0\n",
      "295  0.083178 -0.243602  0.357564 -0.098703 -0.227464  0.0\n",
      "157  0.055724 -0.163200 -1.133110 -0.066126 -0.152388  0.0\n",
      "238  0.126452 -0.370340 -0.064334 -0.150055 -0.345806  0.0\n",
      "17   0.118478 -0.346988 -0.139005 -0.140593 -0.324001  0.0\n",
      "164  0.139004 -0.407100  1.283321 -0.164949 -0.380130  0.0\n",
      "33   0.056930 -0.166730 -0.928395 -0.067556 -0.155685  0.0\n",
      "24   0.097531 -0.285638 -0.093401 -0.115735 -0.266715  1.0\n",
      "215  0.038883 -0.113877  0.670765 -0.046141 -0.106333  0.0\n",
      "119 -0.259853  0.761033  2.332351  0.308357  0.710616  1.0\n",
      "7    0.276915 -0.811002  0.296344 -0.328603 -0.757274  1.0\n",
      "90   0.040208 -0.117756  0.651974 -0.047713 -0.109955  0.0\n",
      "46  -0.013564  0.039725 -0.537653  0.016096  0.037093  0.0\n",
      "73   0.043639 -0.127807 -0.086592 -0.051785 -0.119340  0.0\n",
      "93   0.068588 -0.200875 -0.513328 -0.081391 -0.187567  0.0\n",
      "76   0.107918 -0.316060  1.713443 -0.128062 -0.295122  0.0\n",
      "286  0.010558 -0.030922  0.149024 -0.012529 -0.028873  0.0\n",
      "60   0.062228 -0.182247  0.996821 -0.073843 -0.170173  0.0\n",
      "77   0.085434 -0.250212 -1.137772 -0.101381 -0.233636  0.0\n",
      "63  -0.048642  0.142458  0.976490  0.057721  0.133021  0.0\n",
      "234 -0.034605  0.101346  0.172010  0.041064  0.094632  0.0\n",
      "229  0.150320 -0.440243 -0.254723 -0.178379 -0.411078  0.0\n",
      "111  0.036719 -0.107539  0.529002 -0.043573 -0.100415  0.0\n",
      "231  0.037014 -0.108403 -1.029966 -0.043923 -0.101222  0.0\n",
      "180  0.151902 -0.444875  2.230666 -0.180255 -0.415403  0.0\n",
      "144  0.048781 -0.142866 -0.849575 -0.057887 -0.133401  0.0\n",
      "239 -0.037728  0.110494  1.300327  0.044770  0.103174  0.0\n",
      "75   0.042747 -0.125194  0.523264 -0.050726 -0.116900  0.0\n",
      "297  0.146055 -0.427751 -1.035479 -0.173317 -0.399413  0.0\n",
      "278  0.120725 -0.353567 -0.815155 -0.143259 -0.330144  0.0\n",
      "97   0.036656 -0.107353  1.624771 -0.043498 -0.100241  0.0\n",
      "92   0.048539 -0.142156 -1.564186 -0.057599 -0.132739  0.0\n",
      "192  0.055258 -0.161834  0.921478 -0.065572 -0.151113  0.0\n",
      "25   0.034777 -0.101853  0.407796 -0.041269 -0.095105  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into a train set = 80% and test = 20%\n",
    "data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#Print the shape of the train and test set\n",
    "print(f\"Train Data shape: \\n{data_train}\\n\")\n",
    "print(f\"Test Data shape: \\n{data_test}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_train: \n",
      "       5\n",
      "232  0.0\n",
      "59   0.0\n",
      "6    0.0\n",
      "185  1.0\n",
      "173  1.0\n",
      "..   ...\n",
      "188  0.0\n",
      "71   1.0\n",
      "106  0.0\n",
      "270  0.0\n",
      "102  0.0\n",
      "\n",
      "[240 rows x 1 columns]\n",
      "\n",
      "labels_test: \n",
      "       5\n",
      "203  1.0\n",
      "266  1.0\n",
      "152  0.0\n",
      "9    1.0\n",
      "233  0.0\n",
      "226  1.0\n",
      "196  0.0\n",
      "109  0.0\n",
      "5    0.0\n",
      "175  0.0\n",
      "237  0.0\n",
      "57   0.0\n",
      "218  1.0\n",
      "45   0.0\n",
      "182  0.0\n",
      "221  0.0\n",
      "289  0.0\n",
      "211  0.0\n",
      "148  1.0\n",
      "165  0.0\n",
      "78   0.0\n",
      "113  1.0\n",
      "249  0.0\n",
      "250  0.0\n",
      "104  0.0\n",
      "42   0.0\n",
      "281  0.0\n",
      "295  0.0\n",
      "157  0.0\n",
      "238  0.0\n",
      "17   0.0\n",
      "164  0.0\n",
      "33   0.0\n",
      "24   1.0\n",
      "215  0.0\n",
      "119  1.0\n",
      "7    1.0\n",
      "90   0.0\n",
      "46   0.0\n",
      "73   0.0\n",
      "93   0.0\n",
      "76   0.0\n",
      "286  0.0\n",
      "60   0.0\n",
      "77   0.0\n",
      "63   0.0\n",
      "234  0.0\n",
      "229  0.0\n",
      "111  0.0\n",
      "231  0.0\n",
      "180  0.0\n",
      "144  0.0\n",
      "239  0.0\n",
      "75   0.0\n",
      "297  0.0\n",
      "278  0.0\n",
      "97   0.0\n",
      "92   0.0\n",
      "192  0.0\n",
      "25   0.0\n",
      "\n",
      "features_train: \n",
      "            0         1         2         3         4\n",
      "232 -0.019817  0.058038 -1.455309  0.023516  0.054193\n",
      "59   0.055881 -0.163657  0.666498 -0.066311 -0.152815\n",
      "6    0.041093 -0.120351  1.869881 -0.048764 -0.112377\n",
      "185  0.148608 -0.435229 -0.695417 -0.176347 -0.406396\n",
      "173 -0.013564  0.039726  0.225626  0.016096  0.037094\n",
      "..        ...       ...       ...       ...       ...\n",
      "188  0.078593 -0.230175 -0.700165 -0.093263 -0.214926\n",
      "71  -0.016761  0.049087 -1.858915  0.019889  0.045835\n",
      "106  0.047683 -0.139649 -0.473959 -0.056583 -0.130397\n",
      "270  0.147648 -0.432416 -0.864888 -0.175207 -0.403769\n",
      "102 -0.004327  0.012672  0.695998  0.005134  0.011832\n",
      "\n",
      "[240 rows x 5 columns]\n",
      "\n",
      "lfeatures_test: \n",
      "            0         1         2         3         4\n",
      "203 -0.321374  0.941208  0.287581  0.381360  0.878854\n",
      "266  0.264000 -0.773176  0.919191 -0.313277 -0.721955\n",
      "152  0.027106 -0.079385  1.432811 -0.032165 -0.074126\n",
      "9   -0.251536  0.736675  0.503760  0.298487  0.687871\n",
      "233 -0.041580  0.121775  0.800144  0.049341  0.113708\n",
      "226 -0.041806  0.122438  0.169571  0.049609  0.114326\n",
      "196  0.039404 -0.115401  0.260380 -0.046759 -0.107756\n",
      "109  0.082851 -0.242646 -1.106080 -0.098316 -0.226571\n",
      "5    0.062787 -0.183884  0.826365 -0.074507 -0.171702\n",
      "175 -0.003825  0.011204 -1.924100  0.004539  0.010461\n",
      "237  0.083398 -0.244248  1.467195 -0.098965 -0.228067\n",
      "57   0.051158 -0.149827 -0.406991 -0.060707 -0.139901\n",
      "218 -0.482802  1.413983 -0.439613  0.572920  1.320309\n",
      "45   0.042370 -0.124090 -1.033826 -0.050279 -0.115870\n",
      "182  0.064293 -0.188295 -0.252846 -0.076294 -0.175821\n",
      "221  0.031865 -0.093322  0.691377 -0.037813 -0.087140\n",
      "289 -0.016949  0.049639  0.714885  0.020113  0.046350\n",
      "211  0.018044 -0.052847  0.948641 -0.021412 -0.049346\n",
      "148  0.102059 -0.298900  1.294859 -0.121109 -0.279098\n",
      "165  0.014979 -0.043869 -1.333941 -0.017775 -0.040963\n",
      "78   0.029495 -0.086381 -0.786875 -0.035000 -0.080658\n",
      "113  0.015796 -0.046261  0.652778 -0.018744 -0.043197\n",
      "249  0.149643 -0.438261 -0.026008 -0.177575 -0.409227\n",
      "250 -0.055383  0.162199  0.100424  0.065720  0.151454\n",
      "104  0.047499 -0.139110  0.879414 -0.056365 -0.129894\n",
      "42   0.051770 -0.151619  0.138218 -0.061433 -0.141575\n",
      "281  0.155715 -0.456042 -1.154290 -0.184780 -0.425830\n",
      "295  0.083178 -0.243602  0.357564 -0.098703 -0.227464\n",
      "157  0.055724 -0.163200 -1.133110 -0.066126 -0.152388\n",
      "238  0.126452 -0.370340 -0.064334 -0.150055 -0.345806\n",
      "17   0.118478 -0.346988 -0.139005 -0.140593 -0.324001\n",
      "164  0.139004 -0.407100  1.283321 -0.164949 -0.380130\n",
      "33   0.056930 -0.166730 -0.928395 -0.067556 -0.155685\n",
      "24   0.097531 -0.285638 -0.093401 -0.115735 -0.266715\n",
      "215  0.038883 -0.113877  0.670765 -0.046141 -0.106333\n",
      "119 -0.259853  0.761033  2.332351  0.308357  0.710616\n",
      "7    0.276915 -0.811002  0.296344 -0.328603 -0.757274\n",
      "90   0.040208 -0.117756  0.651974 -0.047713 -0.109955\n",
      "46  -0.013564  0.039725 -0.537653  0.016096  0.037093\n",
      "73   0.043639 -0.127807 -0.086592 -0.051785 -0.119340\n",
      "93   0.068588 -0.200875 -0.513328 -0.081391 -0.187567\n",
      "76   0.107918 -0.316060  1.713443 -0.128062 -0.295122\n",
      "286  0.010558 -0.030922  0.149024 -0.012529 -0.028873\n",
      "60   0.062228 -0.182247  0.996821 -0.073843 -0.170173\n",
      "77   0.085434 -0.250212 -1.137772 -0.101381 -0.233636\n",
      "63  -0.048642  0.142458  0.976490  0.057721  0.133021\n",
      "234 -0.034605  0.101346  0.172010  0.041064  0.094632\n",
      "229  0.150320 -0.440243 -0.254723 -0.178379 -0.411078\n",
      "111  0.036719 -0.107539  0.529002 -0.043573 -0.100415\n",
      "231  0.037014 -0.108403 -1.029966 -0.043923 -0.101222\n",
      "180  0.151902 -0.444875  2.230666 -0.180255 -0.415403\n",
      "144  0.048781 -0.142866 -0.849575 -0.057887 -0.133401\n",
      "239 -0.037728  0.110494  1.300327  0.044770  0.103174\n",
      "75   0.042747 -0.125194  0.523264 -0.050726 -0.116900\n",
      "297  0.146055 -0.427751 -1.035479 -0.173317 -0.399413\n",
      "278  0.120725 -0.353567 -0.815155 -0.143259 -0.330144\n",
      "97   0.036656 -0.107353  1.624771 -0.043498 -0.100241\n",
      "92   0.048539 -0.142156 -1.564186 -0.057599 -0.132739\n",
      "192  0.055258 -0.161834  0.921478 -0.065572 -0.151113\n",
      "25   0.034777 -0.101853  0.407796 -0.041269 -0.095105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting Labels\n",
    "#Get a list of all columns\n",
    "columns = data_train.columns.to_list()\n",
    "#Remove the label and save it\n",
    "columns_drop = columns.pop(-1)\n",
    "\n",
    "#Remove all labels except for the label in the train and test dataframe\n",
    "labels_train = data_train.drop(columns, axis=1)\n",
    "labels_test = data_test.drop(columns, axis=1)\n",
    "\n",
    "#Print the labesl of the test and train\n",
    "print(f\"labels_train: \\n{labels_train}\\n\")\n",
    "print(f\"labels_test: \\n{labels_test}\\n\")\n",
    "\n",
    "#Remove the label from the train and test dataframe\n",
    "features_train = data_train.drop(['5'], axis=1)\n",
    "features_test = data_test.drop(['5'], axis=1)\n",
    "\n",
    "#Print the features of the train and test dataset\n",
    "print(f\"features_train: \\n{features_train }\\n\")\n",
    "print(f\"lfeatures_test: \\n{features_test }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTrainFlat = labels_train.values.ravel()\n",
    "\n",
    "#Fit one vs rest Gradient Boosting classification\n",
    "gradientBoosting = GradientBoostingClassifier()\n",
    "gradientBoosting = gradientBoosting.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Fit RandomForestClassifier classification\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest = randomForest.fit(features_train,labelTrainFlat)\n",
    "\n",
    "#Create a KNeighbors classification object\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors = kNeighbors.fit(features_train,labelTrainFlat)\n",
    "\n",
    "#Create an LogisticRegression object\n",
    "logisticRegression = LogisticRegression(max_iter=5000)\n",
    "logisticRegression = logisticRegression.fit(features_train,labelTrainFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters GradientBoosting: \n",
      "{'learning_rate': 0.44, 'min_samples_leaf': 7, 'min_samples_split': 7, 'n_estimators': 59}\n",
      "\n",
      "Best estimator GradientBoosting: \n",
      "GradientBoostingClassifier(learning_rate=0.44, min_samples_leaf=7,\n",
      "                           min_samples_split=7, n_estimators=59)\n",
      "\n",
      "Best score GradientBoosting: \n",
      "0.6533534322820038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of GradientBoosting for GridSearchCV\n",
    "parametersGradientBoosting = [\n",
    "    {'learning_rate': [0.44,0.45,0.46],'min_samples_leaf': [5,6,7],'min_samples_split': [7,8,9,10], 'n_estimators': [57,58,59,60]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchGradientBoosting = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the GradientBoosting \n",
    "grid_searchGradientBoosting.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters GradientBoosting: \\n{grid_searchGradientBoosting.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting: \\n{grid_searchGradientBoosting.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting: \\n{grid_searchGradientBoosting.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 1, 'p': 1, 'weights': 'uniform'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=1, p=1)\n",
      "\n",
      "Best score KNeighbors: \n",
      "0.6089795918367347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of KNeighbors for GridSearchCV\n",
    "parametersKNeighbors = [\n",
    "    {'n_neighbors': [1,2,3],'weights':['uniform', 'distance'],'algorithm':['auto'], 'p': [1,2,3]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "#Preform KNeighbors to find best parameters\n",
    "grid_searchKNeighbors = GridSearchCV(kNeighbors, parametersKNeighbors, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the KNeighbors \n",
    "grid_searchKNeighbors.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters KNeighbors: \\n{grid_searchKNeighbors.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors: \\n{grid_searchKNeighbors.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors: \\n{grid_searchKNeighbors.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters Logistic Regression: \n",
      "{'C': 1, 'multi_class': 'ovr', 'penalty': 'none'}\n",
      "\n",
      "Best estimator Logistic Regression: \n",
      "LogisticRegression(C=1, max_iter=5000, multi_class='ovr', penalty='none')\n",
      "\n",
      "Best score Logistic Regression: \n",
      "0.5668181818181818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of LogisticRegression for GridSearchCV\n",
    "parametersLogisticRegression = [\n",
    "    {'multi_class': ['ovr'],'penalty':['none','l2'], 'C': [1,2,3]}\n",
    "]\n",
    "scoringX = {\"accuracy\": \"accuracy\", \"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "\n",
    "#Preform LogisticRegression to find best parameters\n",
    "grid_searchLogisticRegression = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the LogisticRegression \n",
    "grid_searchLogisticRegression.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print LogisticRegression Results\n",
    "print(f\"Best parameters Logistic Regression: \\n{grid_searchLogisticRegression.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression: \\n{grid_searchLogisticRegression.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression: \\n{grid_searchLogisticRegression.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters RandomForest: \n",
      "{'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_split': 0.05, 'n_estimators': 145}\n",
      "\n",
      "Best estimator RandomForest: \n",
      "RandomForestClassifier(bootstrap=False, max_depth=10, max_features='auto',\n",
      "                       min_samples_split=0.05, n_estimators=145)\n",
      "\n",
      "Best score RandomForest: \n",
      "0.6154220779220779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\anaconda3\\envs\\csi4106\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of RandomForest for GridSearchCV\n",
    "parametersRandomForest = [\n",
    "    {'n_estimators': [145,150,155,190],'max_depth': [10,12], 'bootstrap': [True, False],\n",
    "     'min_samples_split': [0.05,2], 'max_features': ['auto']}\n",
    "]\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchRandomForest = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the RandomForest \n",
    "grid_searchRandomForest.fit(features_train, labelTrainFlat)\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters RandomForest: \\n{grid_searchRandomForest.best_params_}\\n\")\n",
    "print(f\"Best estimator RandomForest: \\n{grid_searchRandomForest.best_estimator_}\\n\")\n",
    "print(f\"Best score RandomForest: \\n{grid_searchRandomForest.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Accuracy for Gradient Boosting: \n",
      "0.8096064814814815\n",
      "\n",
      "Balanced Test Accuracy for Gradient Boosting: \n",
      "0.6155031179138322\n",
      "\n",
      "Mean F1 Macro for Gradient Boosting: \n",
      "0.6237543985452254\n",
      "\n",
      "Mean Test Accuracy for Random Forests: \n",
      "0.7924479166666667\n",
      "\n",
      "Balanced Test Accuracy for Random Forests: \n",
      "0.5976456690630798\n",
      "\n",
      "Mean F1 Macro for Random Forests: \n",
      "0.6069664642340505\n",
      "\n",
      "Mean Test Accuracy for Logistic Regression: \n",
      "0.8409722222222222\n",
      "\n",
      "Balanced Test Accuracy for Logistic Regression: \n",
      "0.5634469696969697\n",
      "\n",
      "Mean F1 Macro for Logistic Regression: \n",
      "0.563857403016725\n",
      "\n",
      "Mean Test Accuracy for K Nearest Neighbours: \n",
      "0.7932870370370371\n",
      "\n",
      "Balanced Test Accuracy for K Nearest Neighbours: \n",
      "0.588716244073387\n",
      "\n",
      "Mean F1 Macro for Logistic K Nearest Neighbours: \n",
      "0.5906048398111855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the results for all classifiers \n",
    "cross_val_resultsGB = grid_searchGradientBoosting.cv_results_\n",
    "cross_val_resultsRF = grid_searchRandomForest.cv_results_\n",
    "cross_val_resultsLR = grid_searchLogisticRegression.cv_results_\n",
    "cross_val_resultsKN = grid_searchKNeighbors.cv_results_\n",
    "\n",
    "\n",
    "#Print the results of all classiifiers\n",
    "#GBC\n",
    "print(f\"Mean Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_F1_macro'])}\\n\")\n",
    "\n",
    "#RFC\n",
    "print(f\"Mean Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_F1_macro'])}\\n\")\n",
    "\n",
    "#LRC\n",
    "print(f\"Mean Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_F1_macro'])}\\n\")\n",
    "\n",
    "#KNC\n",
    "print(f\"Mean Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_accuracy'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_bal_accuracy'])}\\n\")\n",
    "print(f\"Mean F1 Macro for Logistic K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_F1_macro'])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ea9996f2a91c7d112e626959c304b606e4bf2254e73fec145d965796b2ca69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
