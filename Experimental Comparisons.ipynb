{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, cross_val_score, GridSearchCV, cross_validate\n",
    "\n",
    "# importing two different imputation methods that take into consideration all the features when predicting the missing values\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#multiclass imports\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.dummy import DummyClassifier #Will identify the maority calss base line, model needs to do better then the baseline\n",
    "\n",
    "from statistics import mean\n",
    "# to reduce randomness then you put the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "from ArtificialImmuneSystem import *\n",
    "from imblearn.metrics import geometric_mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data\\GeneratedSyntheticData-testing.csv')\n",
    "#df = df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: \n",
      "(300, 6)\n",
      "\n",
      "Data size: \n",
      "1800\n",
      "\n",
      "Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Old Class Distribution: Counter({0.0: 247, 1.0: 53})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data shape: \\n{df.shape}\\n\")\n",
    "print(f\"Data size: \\n{df.size}\\n\")\n",
    "print(f\"Data ndim: \\n{df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Old Class Distribution: {Counter(df['5'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data shape: \n",
      "            0         1         2         3         4    5\n",
      "232 -0.019817  0.058038 -1.455309  0.023516  0.054193  0.0\n",
      "59   0.055881 -0.163657  0.666498 -0.066311 -0.152815  0.0\n",
      "6    0.041093 -0.120351  1.869881 -0.048764 -0.112377  0.0\n",
      "185  0.148608 -0.435229 -0.695417 -0.176347 -0.406396  1.0\n",
      "173 -0.013564  0.039726  0.225626  0.016096  0.037094  1.0\n",
      "..        ...       ...       ...       ...       ...  ...\n",
      "188  0.078593 -0.230175 -0.700165 -0.093263 -0.214926  0.0\n",
      "71  -0.016761  0.049087 -1.858915  0.019889  0.045835  1.0\n",
      "106  0.047683 -0.139649 -0.473959 -0.056583 -0.130397  0.0\n",
      "270  0.147648 -0.432416 -0.864888 -0.175207 -0.403769  0.0\n",
      "102 -0.004327  0.012672  0.695998  0.005134  0.011832  0.0\n",
      "\n",
      "[240 rows x 6 columns]\n",
      "\n",
      "Test Data shape: \n",
      "            0         1         2         3         4    5\n",
      "203 -0.321374  0.941208  0.287581  0.381360  0.878854  1.0\n",
      "266  0.264000 -0.773176  0.919191 -0.313277 -0.721955  1.0\n",
      "152  0.027106 -0.079385  1.432811 -0.032165 -0.074126  0.0\n",
      "9   -0.251536  0.736675  0.503760  0.298487  0.687871  1.0\n",
      "233 -0.041580  0.121775  0.800144  0.049341  0.113708  0.0\n",
      "226 -0.041806  0.122438  0.169571  0.049609  0.114326  1.0\n",
      "196  0.039404 -0.115401  0.260380 -0.046759 -0.107756  0.0\n",
      "109  0.082851 -0.242646 -1.106080 -0.098316 -0.226571  0.0\n",
      "5    0.062787 -0.183884  0.826365 -0.074507 -0.171702  0.0\n",
      "175 -0.003825  0.011204 -1.924100  0.004539  0.010461  0.0\n",
      "237  0.083398 -0.244248  1.467195 -0.098965 -0.228067  0.0\n",
      "57   0.051158 -0.149827 -0.406991 -0.060707 -0.139901  0.0\n",
      "218 -0.482802  1.413983 -0.439613  0.572920  1.320309  1.0\n",
      "45   0.042370 -0.124090 -1.033826 -0.050279 -0.115870  0.0\n",
      "182  0.064293 -0.188295 -0.252846 -0.076294 -0.175821  0.0\n",
      "221  0.031865 -0.093322  0.691377 -0.037813 -0.087140  0.0\n",
      "289 -0.016949  0.049639  0.714885  0.020113  0.046350  0.0\n",
      "211  0.018044 -0.052847  0.948641 -0.021412 -0.049346  0.0\n",
      "148  0.102059 -0.298900  1.294859 -0.121109 -0.279098  1.0\n",
      "165  0.014979 -0.043869 -1.333941 -0.017775 -0.040963  0.0\n",
      "78   0.029495 -0.086381 -0.786875 -0.035000 -0.080658  0.0\n",
      "113  0.015796 -0.046261  0.652778 -0.018744 -0.043197  1.0\n",
      "249  0.149643 -0.438261 -0.026008 -0.177575 -0.409227  0.0\n",
      "250 -0.055383  0.162199  0.100424  0.065720  0.151454  0.0\n",
      "104  0.047499 -0.139110  0.879414 -0.056365 -0.129894  0.0\n",
      "42   0.051770 -0.151619  0.138218 -0.061433 -0.141575  0.0\n",
      "281  0.155715 -0.456042 -1.154290 -0.184780 -0.425830  0.0\n",
      "295  0.083178 -0.243602  0.357564 -0.098703 -0.227464  0.0\n",
      "157  0.055724 -0.163200 -1.133110 -0.066126 -0.152388  0.0\n",
      "238  0.126452 -0.370340 -0.064334 -0.150055 -0.345806  0.0\n",
      "17   0.118478 -0.346988 -0.139005 -0.140593 -0.324001  0.0\n",
      "164  0.139004 -0.407100  1.283321 -0.164949 -0.380130  0.0\n",
      "33   0.056930 -0.166730 -0.928395 -0.067556 -0.155685  0.0\n",
      "24   0.097531 -0.285638 -0.093401 -0.115735 -0.266715  1.0\n",
      "215  0.038883 -0.113877  0.670765 -0.046141 -0.106333  0.0\n",
      "119 -0.259853  0.761033  2.332351  0.308357  0.710616  1.0\n",
      "7    0.276915 -0.811002  0.296344 -0.328603 -0.757274  1.0\n",
      "90   0.040208 -0.117756  0.651974 -0.047713 -0.109955  0.0\n",
      "46  -0.013564  0.039725 -0.537653  0.016096  0.037093  0.0\n",
      "73   0.043639 -0.127807 -0.086592 -0.051785 -0.119340  0.0\n",
      "93   0.068588 -0.200875 -0.513328 -0.081391 -0.187567  0.0\n",
      "76   0.107918 -0.316060  1.713443 -0.128062 -0.295122  0.0\n",
      "286  0.010558 -0.030922  0.149024 -0.012529 -0.028873  0.0\n",
      "60   0.062228 -0.182247  0.996821 -0.073843 -0.170173  0.0\n",
      "77   0.085434 -0.250212 -1.137772 -0.101381 -0.233636  0.0\n",
      "63  -0.048642  0.142458  0.976490  0.057721  0.133021  0.0\n",
      "234 -0.034605  0.101346  0.172010  0.041064  0.094632  0.0\n",
      "229  0.150320 -0.440243 -0.254723 -0.178379 -0.411078  0.0\n",
      "111  0.036719 -0.107539  0.529002 -0.043573 -0.100415  0.0\n",
      "231  0.037014 -0.108403 -1.029966 -0.043923 -0.101222  0.0\n",
      "180  0.151902 -0.444875  2.230666 -0.180255 -0.415403  0.0\n",
      "144  0.048781 -0.142866 -0.849575 -0.057887 -0.133401  0.0\n",
      "239 -0.037728  0.110494  1.300327  0.044770  0.103174  0.0\n",
      "75   0.042747 -0.125194  0.523264 -0.050726 -0.116900  0.0\n",
      "297  0.146055 -0.427751 -1.035479 -0.173317 -0.399413  0.0\n",
      "278  0.120725 -0.353567 -0.815155 -0.143259 -0.330144  0.0\n",
      "97   0.036656 -0.107353  1.624771 -0.043498 -0.100241  0.0\n",
      "92   0.048539 -0.142156 -1.564186 -0.057599 -0.132739  0.0\n",
      "192  0.055258 -0.161834  0.921478 -0.065572 -0.151113  0.0\n",
      "25   0.034777 -0.101853  0.407796 -0.041269 -0.095105  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset into a train set = 80% and test = 20%\n",
    "data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#Print the shape of the train and test set\n",
    "print(f\"Train Data shape: \\n{data_train}\\n\")\n",
    "print(f\"Test Data shape: \\n{data_test}\\n\")\n",
    "\n",
    "data_train_AIS = data_train.copy()\n",
    "data_train_SMOTE = data_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1: 0.596569377069377\n",
      "score2: 0.6354592468722904\n",
      "score1: 0.6354592468722904\n",
      "score2: 0.6543194672505017\n",
      "score1: 0.6543194672505017\n",
      "score2: 0.5652986502986502\n",
      "score1: 0.6543194672505017\n",
      "score2: 0.577116448432238\n",
      "score1: 0.6543194672505017\n",
      "score2: 0.6662830687830688\n",
      "score1: 0.6662830687830688\n",
      "score2: 0.6164073981815917\n",
      "score1: 0.6662830687830688\n",
      "score2: 0.6341775841775842\n",
      "score1: 0.6662830687830688\n",
      "score2: 0.570979671979672\n",
      "score1: 0.6662830687830688\n",
      "score2: 0.5845366345366345\n",
      "score1: 0.6662830687830688\n",
      "score2: 0.6060785510785511\n",
      "SMOTE Oversampled Data shape: \n",
      "(394, 6)\n",
      "\n",
      "SMOTE Oversampled Data size: \n",
      "2364\n",
      "\n",
      "SMOTE Oversampled Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "AIS Oversampled Data shape: \n",
      "(394, 6)\n",
      "\n",
      "AIS Oversampled Data size: \n",
      "2364\n",
      "\n",
      "AIS Oversampled Data ndim: \n",
      "2\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "New SMOTE Class Distribution: Counter({0.0: 197, 1.0: 197})\n",
      "New AIS Class Distribution: Counter({0.0: 197, 1.0: 197})\n",
      "Old Class Distribution: Counter({0.0: 247, 1.0: 53})\n",
      "_____________________________________________\n",
      "\n",
      "            0         1         2         3         4\n",
      "0   -0.019817  0.058038 -1.455309  0.023516  0.054193\n",
      "1    0.055881 -0.163657  0.666498 -0.066311 -0.152815\n",
      "2    0.041093 -0.120351  1.869881 -0.048764 -0.112377\n",
      "3    0.148608 -0.435229 -0.695417 -0.176347 -0.406396\n",
      "4   -0.013564  0.039726  0.225626  0.016096  0.037094\n",
      "..        ...       ...       ...       ...       ...\n",
      "389  0.089128 -0.261029  0.479138 -0.105764 -0.243736\n",
      "390  0.069725 -0.204204 -0.571140 -0.082740 -0.190676\n",
      "391  0.102146 -0.299156  1.877256 -0.121213 -0.279338\n",
      "392  0.123306 -0.361126  0.007884 -0.146322 -0.337202\n",
      "393 -0.228903  0.670389  0.977944  0.271629  0.625977\n",
      "\n",
      "[394 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Create an oversampling object\n",
    "oversample = SMOTE()\n",
    "oversample_AIS = ArtificialImmuneSystem()\n",
    "#Oversample and add to the dataframe to fix the class imbalance\n",
    "randomForest = RandomForestClassifier()\n",
    "x_over, y_over = oversample.fit_resample(data_train_SMOTE.drop([\"5\"], axis=1), data_train_SMOTE.drop(data_train_SMOTE.columns[0:-1],axis=1))\n",
    "input_x_over_AIS, y_over_AIS = oversample_AIS.AIS_Resample(data_train_AIS.drop([\"5\"], axis=1), data_train_AIS.drop(data_train_AIS.columns[0:-1],axis=1), 20, 5, randomForest,5,'balanced_accuracy' )\n",
    "\n",
    "smote_df = pd.concat([x_over, y_over], axis=1)\n",
    "ais_df = pd.concat([input_x_over_AIS, y_over_AIS], axis=1)\n",
    "\n",
    "# print the dimensionality of the oversampled dataset\n",
    "print(f\"SMOTE Oversampled Data shape: \\n{smote_df.shape}\\n\")\n",
    "print(f\"SMOTE Oversampled Data size: \\n{smote_df.size}\\n\")\n",
    "print(f\"SMOTE Oversampled Data ndim: \\n{smote_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "# print the dimensionality of the oversampled dataset\n",
    "print(f\"AIS Oversampled Data shape: \\n{ais_df.shape}\\n\")\n",
    "print(f\"AIS Oversampled Data size: \\n{ais_df.size}\\n\")\n",
    "print(f\"AIS Oversampled Data ndim: \\n{ais_df.ndim}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "\n",
    "\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"New SMOTE Class Distribution: {Counter(smote_df['5'])}\")\n",
    "print(f\"New AIS Class Distribution: {Counter(ais_df['5'])}\")\n",
    "# print the new class distribution using a Counter\n",
    "print(f\"Old Class Distribution: {Counter(df['5'])}\")\n",
    "\n",
    "print(\"_____________________________________________\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aisOversample = ArtificialImmuneSystem()\n",
    "#minority_class = df[df['5'] == 1]\n",
    "#majority_class = df[df['5'] == 0]\n",
    "\n",
    "#requiredPopulation = len(majority_class)-len(minority_class)\n",
    "#population = aisOversample.AIS(minority_class, max_rounds=100, totalPopulation=requiredPopulation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_train: \n",
      "       5\n",
      "232  0.0\n",
      "59   0.0\n",
      "6    0.0\n",
      "185  1.0\n",
      "173  1.0\n",
      "..   ...\n",
      "188  0.0\n",
      "71   1.0\n",
      "106  0.0\n",
      "270  0.0\n",
      "102  0.0\n",
      "\n",
      "[240 rows x 1 columns]\n",
      "\n",
      "labels_test: \n",
      "       5\n",
      "203  1.0\n",
      "266  1.0\n",
      "152  0.0\n",
      "9    1.0\n",
      "233  0.0\n",
      "226  1.0\n",
      "196  0.0\n",
      "109  0.0\n",
      "5    0.0\n",
      "175  0.0\n",
      "237  0.0\n",
      "57   0.0\n",
      "218  1.0\n",
      "45   0.0\n",
      "182  0.0\n",
      "221  0.0\n",
      "289  0.0\n",
      "211  0.0\n",
      "148  1.0\n",
      "165  0.0\n",
      "78   0.0\n",
      "113  1.0\n",
      "249  0.0\n",
      "250  0.0\n",
      "104  0.0\n",
      "42   0.0\n",
      "281  0.0\n",
      "295  0.0\n",
      "157  0.0\n",
      "238  0.0\n",
      "17   0.0\n",
      "164  0.0\n",
      "33   0.0\n",
      "24   1.0\n",
      "215  0.0\n",
      "119  1.0\n",
      "7    1.0\n",
      "90   0.0\n",
      "46   0.0\n",
      "73   0.0\n",
      "93   0.0\n",
      "76   0.0\n",
      "286  0.0\n",
      "60   0.0\n",
      "77   0.0\n",
      "63   0.0\n",
      "234  0.0\n",
      "229  0.0\n",
      "111  0.0\n",
      "231  0.0\n",
      "180  0.0\n",
      "144  0.0\n",
      "239  0.0\n",
      "75   0.0\n",
      "297  0.0\n",
      "278  0.0\n",
      "97   0.0\n",
      "92   0.0\n",
      "192  0.0\n",
      "25   0.0\n",
      "\n",
      "features_train: \n",
      "            0         1         2         3         4\n",
      "232 -0.019817  0.058038 -1.455309  0.023516  0.054193\n",
      "59   0.055881 -0.163657  0.666498 -0.066311 -0.152815\n",
      "6    0.041093 -0.120351  1.869881 -0.048764 -0.112377\n",
      "185  0.148608 -0.435229 -0.695417 -0.176347 -0.406396\n",
      "173 -0.013564  0.039726  0.225626  0.016096  0.037094\n",
      "..        ...       ...       ...       ...       ...\n",
      "188  0.078593 -0.230175 -0.700165 -0.093263 -0.214926\n",
      "71  -0.016761  0.049087 -1.858915  0.019889  0.045835\n",
      "106  0.047683 -0.139649 -0.473959 -0.056583 -0.130397\n",
      "270  0.147648 -0.432416 -0.864888 -0.175207 -0.403769\n",
      "102 -0.004327  0.012672  0.695998  0.005134  0.011832\n",
      "\n",
      "[240 rows x 5 columns]\n",
      "\n",
      "lfeatures_test: \n",
      "            0         1         2         3         4\n",
      "203 -0.321374  0.941208  0.287581  0.381360  0.878854\n",
      "266  0.264000 -0.773176  0.919191 -0.313277 -0.721955\n",
      "152  0.027106 -0.079385  1.432811 -0.032165 -0.074126\n",
      "9   -0.251536  0.736675  0.503760  0.298487  0.687871\n",
      "233 -0.041580  0.121775  0.800144  0.049341  0.113708\n",
      "226 -0.041806  0.122438  0.169571  0.049609  0.114326\n",
      "196  0.039404 -0.115401  0.260380 -0.046759 -0.107756\n",
      "109  0.082851 -0.242646 -1.106080 -0.098316 -0.226571\n",
      "5    0.062787 -0.183884  0.826365 -0.074507 -0.171702\n",
      "175 -0.003825  0.011204 -1.924100  0.004539  0.010461\n",
      "237  0.083398 -0.244248  1.467195 -0.098965 -0.228067\n",
      "57   0.051158 -0.149827 -0.406991 -0.060707 -0.139901\n",
      "218 -0.482802  1.413983 -0.439613  0.572920  1.320309\n",
      "45   0.042370 -0.124090 -1.033826 -0.050279 -0.115870\n",
      "182  0.064293 -0.188295 -0.252846 -0.076294 -0.175821\n",
      "221  0.031865 -0.093322  0.691377 -0.037813 -0.087140\n",
      "289 -0.016949  0.049639  0.714885  0.020113  0.046350\n",
      "211  0.018044 -0.052847  0.948641 -0.021412 -0.049346\n",
      "148  0.102059 -0.298900  1.294859 -0.121109 -0.279098\n",
      "165  0.014979 -0.043869 -1.333941 -0.017775 -0.040963\n",
      "78   0.029495 -0.086381 -0.786875 -0.035000 -0.080658\n",
      "113  0.015796 -0.046261  0.652778 -0.018744 -0.043197\n",
      "249  0.149643 -0.438261 -0.026008 -0.177575 -0.409227\n",
      "250 -0.055383  0.162199  0.100424  0.065720  0.151454\n",
      "104  0.047499 -0.139110  0.879414 -0.056365 -0.129894\n",
      "42   0.051770 -0.151619  0.138218 -0.061433 -0.141575\n",
      "281  0.155715 -0.456042 -1.154290 -0.184780 -0.425830\n",
      "295  0.083178 -0.243602  0.357564 -0.098703 -0.227464\n",
      "157  0.055724 -0.163200 -1.133110 -0.066126 -0.152388\n",
      "238  0.126452 -0.370340 -0.064334 -0.150055 -0.345806\n",
      "17   0.118478 -0.346988 -0.139005 -0.140593 -0.324001\n",
      "164  0.139004 -0.407100  1.283321 -0.164949 -0.380130\n",
      "33   0.056930 -0.166730 -0.928395 -0.067556 -0.155685\n",
      "24   0.097531 -0.285638 -0.093401 -0.115735 -0.266715\n",
      "215  0.038883 -0.113877  0.670765 -0.046141 -0.106333\n",
      "119 -0.259853  0.761033  2.332351  0.308357  0.710616\n",
      "7    0.276915 -0.811002  0.296344 -0.328603 -0.757274\n",
      "90   0.040208 -0.117756  0.651974 -0.047713 -0.109955\n",
      "46  -0.013564  0.039725 -0.537653  0.016096  0.037093\n",
      "73   0.043639 -0.127807 -0.086592 -0.051785 -0.119340\n",
      "93   0.068588 -0.200875 -0.513328 -0.081391 -0.187567\n",
      "76   0.107918 -0.316060  1.713443 -0.128062 -0.295122\n",
      "286  0.010558 -0.030922  0.149024 -0.012529 -0.028873\n",
      "60   0.062228 -0.182247  0.996821 -0.073843 -0.170173\n",
      "77   0.085434 -0.250212 -1.137772 -0.101381 -0.233636\n",
      "63  -0.048642  0.142458  0.976490  0.057721  0.133021\n",
      "234 -0.034605  0.101346  0.172010  0.041064  0.094632\n",
      "229  0.150320 -0.440243 -0.254723 -0.178379 -0.411078\n",
      "111  0.036719 -0.107539  0.529002 -0.043573 -0.100415\n",
      "231  0.037014 -0.108403 -1.029966 -0.043923 -0.101222\n",
      "180  0.151902 -0.444875  2.230666 -0.180255 -0.415403\n",
      "144  0.048781 -0.142866 -0.849575 -0.057887 -0.133401\n",
      "239 -0.037728  0.110494  1.300327  0.044770  0.103174\n",
      "75   0.042747 -0.125194  0.523264 -0.050726 -0.116900\n",
      "297  0.146055 -0.427751 -1.035479 -0.173317 -0.399413\n",
      "278  0.120725 -0.353567 -0.815155 -0.143259 -0.330144\n",
      "97   0.036656 -0.107353  1.624771 -0.043498 -0.100241\n",
      "92   0.048539 -0.142156 -1.564186 -0.057599 -0.132739\n",
      "192  0.055258 -0.161834  0.921478 -0.065572 -0.151113\n",
      "25   0.034777 -0.101853  0.407796 -0.041269 -0.095105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extracting Labels\n",
    "#Get a list of all columns\n",
    "#columns = data_train.columns.to_list()\n",
    "#Remove the label and save it\n",
    "#columns_drop = columns.pop(-1)\n",
    "\n",
    "#Remove all labels except for the label in the train and test dataframe\n",
    "#labels_train = data_train.drop(columns, axis=1)\n",
    "#labels_test = data_test.drop(columns, axis=1)\n",
    "\n",
    "#Print the labesl of the test and train\n",
    "#print(f\"labels_train: \\n{labels_train}\\n\")\n",
    "#print(f\"labels_test: \\n{labels_test}\\n\")\n",
    "\n",
    "#Remove the label from the train and test dataframe\n",
    "#features_train = data_train.drop(['5'], axis=1)\n",
    "#features_test = data_test.drop(['5'], axis=1)\n",
    "\n",
    "#Print the features of the train and test dataset\n",
    "#print(f\"features_train: \\n{features_train }\\n\")\n",
    "#print(f\"lfeatures_test: \\n{features_test }\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelTrainFlat = labels_train.values.ravel()\n",
    "\n",
    "#Fit one vs rest Gradient Boosting classification\n",
    "gradientBoosting = GradientBoostingClassifier()\n",
    "gradientBoosting = gradientBoosting.fit(x_over, y_over.values.ravel())\n",
    "\n",
    "gradientBoosting_AIS = GradientBoostingClassifier()\n",
    "gradientBoosting_AIS = gradientBoosting.fit(input_x_over_AIS, y_over_AIS.values.ravel())\n",
    "\n",
    "#Fit RandomForestClassifier classification\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest = randomForest.fit(x_over,y_over.values.ravel())\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest_AIS  = randomForest.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Create a KNeighbors classification object\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors = kNeighbors.fit(x_over,y_over.values.ravel())\n",
    "\n",
    "kNeighbors = KNeighborsClassifier()\n",
    "kNeighbors_AIS  = kNeighbors.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Create an LogisticRegression object\n",
    "logisticRegression = LogisticRegression(max_iter=5000)\n",
    "logisticRegression = logisticRegression.fit(x_over,y_over.values.ravel())\n",
    "\n",
    "logisticRegression = LogisticRegression(max_iter=5000)\n",
    "logisticRegression_AIS  = logisticRegression.fit(input_x_over_AIS,y_over_AIS.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters GradientBoosting: \n",
      "{'learning_rate': 0.44, 'min_samples_leaf': 7, 'min_samples_split': 7, 'n_estimators': 57}\n",
      "\n",
      "Best estimator GradientBoosting: \n",
      "GradientBoostingClassifier(learning_rate=0.44, min_samples_leaf=7,\n",
      "                           min_samples_split=7, n_estimators=57)\n",
      "\n",
      "Best score GradientBoosting: \n",
      "0.7588775510204082\n",
      "\n",
      "Best parameters GradientBoosting AIS: \n",
      "{'learning_rate': 0.46, 'min_samples_leaf': 5, 'min_samples_split': 9, 'n_estimators': 58}\n",
      "\n",
      "Best estimator GradientBoosting AIS: \n",
      "GradientBoostingClassifier(learning_rate=0.46, min_samples_leaf=5,\n",
      "                           min_samples_split=9, n_estimators=58)\n",
      "\n",
      "Best score GradientBoosting AIS: \n",
      "0.8623979591836735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of GradientBoosting for GridSearchCV\n",
    "parametersGradientBoosting = [\n",
    "    {'learning_rate': [0.44,0.45,0.46],'min_samples_leaf': [5,6,7],'min_samples_split': [7,8,9,10], 'n_estimators': [57,58,59,60]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"roc_auc\": \"roc_auc\", \"bal_accuracy\": \"balanced_accuracy\"}\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchGradientBoosting = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchGradientBoosting_AIS = GridSearchCV(gradientBoosting, parametersGradientBoosting, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the GradientBoosting \n",
    "grid_searchGradientBoosting.fit(x_over, y_over.values.ravel())\n",
    "grid_searchGradientBoosting_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters GradientBoosting: \\n{grid_searchGradientBoosting.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting: \\n{grid_searchGradientBoosting.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting: \\n{grid_searchGradientBoosting.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters GradientBoosting AIS: \\n{grid_searchGradientBoosting_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator GradientBoosting AIS: \\n{grid_searchGradientBoosting_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score GradientBoosting AIS: \\n{grid_searchGradientBoosting_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
      "\n",
      "Best score KNeighbors: \n",
      "0.7614285714285715\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Best parameters KNeighbors: \n",
      "{'algorithm': 'auto', 'n_neighbors': 2, 'p': 3, 'weights': 'uniform'}\n",
      "\n",
      "Best estimator KNeighbors: \n",
      "KNeighborsClassifier(n_neighbors=2, p=3)\n",
      "\n",
      "Best score KNeighbors: \n",
      "0.865\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of KNeighbors for GridSearchCV\n",
    "parametersKNeighbors = [\n",
    "    {'n_neighbors': [1,2,3],'weights':['uniform', 'distance'],'algorithm':['auto'], 'p': [1,2,3]}\n",
    "]\n",
    "\n",
    "#Set the scoring parameters\n",
    "scoringX = {\"roc_auc\": \"roc_auc\", \"bal_accuracy\": \"balanced_accuracy\"}\n",
    "\n",
    "#Preform KNeighbors to find best parameters\n",
    "grid_searchKNeighbors = GridSearchCV(kNeighbors, parametersKNeighbors, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchKNeighbors_AIS = GridSearchCV(kNeighbors, parametersKNeighbors, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the KNeighbors \n",
    "grid_searchKNeighbors.fit(x_over, y_over.values.ravel())\n",
    "grid_searchKNeighbors_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters KNeighbors: \\n{grid_searchKNeighbors.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors: \\n{grid_searchKNeighbors.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors: \\n{grid_searchKNeighbors.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters KNeighbors AIS: \\n{grid_searchKNeighbors_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator KNeighbors AIS: \\n{grid_searchKNeighbors_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score KNeighbors AIS: \\n{grid_searchKNeighbors_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters Logistic Regression: \n",
      "{'C': 1, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\n",
      "Best estimator Logistic Regression: \n",
      "LogisticRegression(C=1, max_iter=5000, multi_class='ovr')\n",
      "\n",
      "Best score Logistic Regression: \n",
      "0.5760204081632654\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Best parameters Logistic Regression AIS: \n",
      "{'C': 1, 'multi_class': 'ovr', 'penalty': 'none'}\n",
      "\n",
      "Best estimator Logistic Regression AIS: \n",
      "LogisticRegression(C=1, max_iter=5000, multi_class='ovr', penalty='none')\n",
      "\n",
      "Best score Logistic Regression AIS: \n",
      "0.7356632653061225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Set the parameters of LogisticRegression for GridSearchCV\n",
    "parametersLogisticRegression = [\n",
    "    {'multi_class': ['ovr'],'penalty':['none','l2'], 'C': [1,2,3]}\n",
    "]\n",
    "scoringX = {\"roc_auc\": \"roc_auc\", \"bal_accuracy\": \"balanced_accuracy\"}\n",
    "\n",
    "#Preform LogisticRegression to find best parameters\n",
    "grid_searchLogisticRegression = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchLogisticRegression_AIS = GridSearchCV(logisticRegression, parametersLogisticRegression, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the LogisticRegression \n",
    "grid_searchLogisticRegression.fit(x_over, y_over.values.ravel())\n",
    "grid_searchLogisticRegression_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print LogisticRegression Results\n",
    "print(f\"Best parameters Logistic Regression: \\n{grid_searchLogisticRegression.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression: \\n{grid_searchLogisticRegression.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression: \\n{grid_searchLogisticRegression.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters Logistic Regression AIS: \\n{grid_searchLogisticRegression_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator Logistic Regression AIS: \\n{grid_searchLogisticRegression_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score Logistic Regression AIS: \\n{grid_searchLogisticRegression_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters RandomForest: \n",
      "{'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_split': 0.05, 'n_estimators': 190}\n",
      "\n",
      "Best estimator RandomForest: \n",
      "RandomForestClassifier(bootstrap=False, max_depth=10, min_samples_split=0.05,\n",
      "                       n_estimators=190)\n",
      "\n",
      "Best score RandomForest: \n",
      "0.746326530612245\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Best parameters RandomForest AIS: \n",
      "{'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_split': 0.05, 'n_estimators': 150}\n",
      "\n",
      "Best estimator RandomForest AIS: \n",
      "RandomForestClassifier(max_depth=10, min_samples_split=0.05, n_estimators=150)\n",
      "\n",
      "Best score RandomForest AIS: \n",
      "0.8981122448979592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the parameters of RandomForest for GridSearchCV\n",
    "parametersRandomForest = [\n",
    "    {'n_estimators': [145,150,155,190],'max_depth': [10,12], 'bootstrap': [True, False],\n",
    "     'min_samples_split': [0.05,2], 'max_features': ['auto']}\n",
    "]\n",
    "\n",
    "#Preform Gridsearch to find best parameters\n",
    "grid_searchRandomForest = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "grid_searchRandomForest_AIS = GridSearchCV(randomForest, parametersRandomForest, cv=4, scoring = scoringX, return_train_score=True, n_jobs=-1, refit='bal_accuracy')\n",
    "\n",
    "#Fit the RandomForest \n",
    "grid_searchRandomForest.fit(x_over, y_over.values.ravel())\n",
    "grid_searchRandomForest_AIS.fit(input_x_over_AIS,y_over_AIS.values.ravel())\n",
    "\n",
    "#Print GridSearchCV Results\n",
    "print(f\"Best parameters RandomForest: \\n{grid_searchRandomForest.best_params_}\\n\")\n",
    "print(f\"Best estimator RandomForest: \\n{grid_searchRandomForest.best_estimator_}\\n\")\n",
    "print(f\"Best score RandomForest: \\n{grid_searchRandomForest.best_score_}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Best parameters RandomForest AIS: \\n{grid_searchRandomForest_AIS.best_params_}\\n\")\n",
    "print(f\"Best estimator RandomForest AIS: \\n{grid_searchRandomForest_AIS.best_estimator_}\\n\")\n",
    "print(f\"Best score RandomForest AIS: \\n{grid_searchRandomForest_AIS.best_score_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Accuracy for Gradient Boosting: \n",
      "0.8266026814637419\n",
      "\n",
      "Balanced Test Accuracy for Gradient Boosting: \n",
      "0.7436337868480726\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Gradient Boosting AIS: \n",
      "0.9116964141098616\n",
      "\n",
      "Balanced Test Accuracy for Gradient Boosting AIS: \n",
      "0.8522293083900226\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Random Forests: \n",
      "0.7946871095376926\n",
      "\n",
      "Balanced Test Accuracy for Random Forests: \n",
      "0.7229607780612245\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Random Forests AIS: \n",
      "0.9183122917534361\n",
      "\n",
      "Balanced Test Accuracy for Random Forests AIS: \n",
      "0.8727551020408163\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Logistic Regression: \n",
      "0.5824836873524919\n",
      "\n",
      "Balanced Test Accuracy for Logistic Regression: \n",
      "0.5726190476190476\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for Logistic Regression AIS: \n",
      "0.6732719005969734\n",
      "\n",
      "Balanced Test Accuracy for Logistic Regression AIS: \n",
      "0.734812925170068\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for K Nearest Neighbours: \n",
      "0.7880133856263594\n",
      "\n",
      "Balanced Test Accuracy for K Nearest Neighbours: \n",
      "0.7410345804988662\n",
      "\n",
      "_____________________________________________\n",
      "\n",
      "Mean Test Accuracy for K Nearest Neighbours AIS: \n",
      "0.8565602295339905\n",
      "\n",
      "Balanced Test Accuracy for K Nearest Neighbours AIS: \n",
      "0.8272335600907029\n",
      "\n",
      "\n",
      "_____________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the results for all classifiers \n",
    "cross_val_resultsGB = grid_searchGradientBoosting.cv_results_\n",
    "cross_val_resultsRF = grid_searchRandomForest.cv_results_\n",
    "cross_val_resultsLR = grid_searchLogisticRegression.cv_results_\n",
    "cross_val_resultsKN = grid_searchKNeighbors.cv_results_\n",
    "\n",
    "cross_val_resultsGB_AIS = grid_searchGradientBoosting_AIS.cv_results_\n",
    "cross_val_resultsRF_AIS = grid_searchRandomForest_AIS.cv_results_\n",
    "cross_val_resultsLR_AIS = grid_searchLogisticRegression_AIS.cv_results_\n",
    "cross_val_resultsKN_AIS = grid_searchKNeighbors_AIS.cv_results_\n",
    "\n",
    "\n",
    "#Print the results of all classiifiers\n",
    "#GBC\n",
    "print(f\"Mean Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Gradient Boosting: \\n{mean(cross_val_resultsGB['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for Gradient Boosting AIS: \\n{mean(cross_val_resultsGB_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Gradient Boosting AIS: \\n{mean(cross_val_resultsGB_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")\n",
    "#RFC\n",
    "print(f\"Mean Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests: \\n{mean(cross_val_resultsRF['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for Random Forests AIS: \\n{mean(cross_val_resultsRF_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Random Forests AIS: \\n{mean(cross_val_resultsRF_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")\n",
    "#LRC\n",
    "print(f\"Mean Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Logistic Regression: \\n{mean(cross_val_resultsLR['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for Logistic Regression AIS: \\n{mean(cross_val_resultsLR_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for Logistic Regression AIS: \\n{mean(cross_val_resultsLR_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")\n",
    "\n",
    "#KNC\n",
    "print(f\"Mean Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for K Nearest Neighbours: \\n{mean(cross_val_resultsKN['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"_____________________________________________\\n\")\n",
    "print(f\"Mean Test Accuracy for K Nearest Neighbours AIS: \\n{mean(cross_val_resultsKN_AIS['mean_test_roc_auc'])}\\n\")\n",
    "print(f\"Balanced Test Accuracy for K Nearest Neighbours AIS: \\n{mean(cross_val_resultsKN_AIS['mean_test_bal_accuracy'])}\\n\")\n",
    "print(\"\\n_____________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "True Positive(TP)  =  6\n",
      "False Positive(FP) =  11\n",
      "True Negative(TN)  =  39\n",
      "False Negative(FN) =  4\n",
      "__________________________________\n",
      "Random Forest\n",
      "True Positive(TP)  =  8\n",
      "False Positive(FP) =  9\n",
      "True Negative(TN)  =  41\n",
      "False Negative(FN) =  2\n",
      "__________________________________\n",
      "Linear Regresion\n",
      "True Positive(TP)  =  6\n",
      "False Positive(FP) =  22\n",
      "True Negative(TN)  =  28\n",
      "False Negative(FN) =  4\n",
      "__________________________________\n",
      "K Neighbors\n",
      "True Positive(TP)  =  4\n",
      "False Positive(FP) =  9\n",
      "True Negative(TN)  =  41\n",
      "False Negative(FN) =  6\n",
      "__________________________________\n",
      "Gradient Boosting AIS\n",
      "True Positive(TP)  =  8\n",
      "False Positive(FP) =  3\n",
      "True Negative(TN)  =  47\n",
      "False Negative(FN) =  2\n",
      "__________________________________\n",
      "Random Forest AIS\n",
      "True Positive(TP)  =  6\n",
      "False Positive(FP) =  0\n",
      "True Negative(TN)  =  50\n",
      "False Negative(FN) =  4\n",
      "__________________________________\n",
      "Linear Regresion AIS\n",
      "True Positive(TP)  =  5\n",
      "False Positive(FP) =  6\n",
      "True Negative(TN)  =  44\n",
      "False Negative(FN) =  5\n",
      "__________________________________\n",
      "K Neighbors AIS\n",
      "True Positive(TP)  =  2\n",
      "False Positive(FP) =  0\n",
      "True Negative(TN)  =  50\n",
      "False Negative(FN) =  8\n",
      "__________________________________\n",
      "Geometric Mean Score for Gradient Boosting: \n",
      "0.6841052550594827\n",
      "\n",
      "Geometric Mean Score for Random Forest: \n",
      "0.8099382692526635\n",
      "\n",
      "Geometric Mean Score for Logestic Regression: \n",
      "0.5796550698475775\n",
      "\n",
      "Geometric Mean Score for K Neighbors: \n",
      "0.5727128425310541\n",
      "\n",
      "Geometric Mean Score for Gradient Boosting AIS: \n",
      "0.867179335547152\n",
      "\n",
      "Geometric Mean Score for Random Forest AIS: \n",
      "0.7745966692414834\n",
      "\n",
      "Geometric Mean Score for Logestic Regression AIS: \n",
      "0.6633249580710799\n",
      "\n",
      "Geometric Mean Score for K Neighbors AIS: \n",
      "0.4472135954999579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_test_over_GB = grid_searchGradientBoosting.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_RF = grid_searchRandomForest.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_LR = grid_searchLogisticRegression.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_KN = grid_searchKNeighbors.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "\n",
    "predictions_test_over_GB_AIS = grid_searchGradientBoosting_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_RF_AIS = grid_searchRandomForest_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_LR_AIS = grid_searchLogisticRegression_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "predictions_test_over_KN_AIS = grid_searchKNeighbors_AIS.best_estimator_.predict(data_test.drop([\"5\"],axis=1))\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB).ravel()\n",
    "print(\"Gradient Boosting\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF).ravel()\n",
    "print(\"Random Forest\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR).ravel()\n",
    "print(\"Linear Regresion\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN).ravel()\n",
    "print(\"K Neighbors\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB_AIS).ravel()\n",
    "print(\"Gradient Boosting AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF_AIS).ravel()\n",
    "print(\"Random Forest AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR_AIS).ravel()\n",
    "print(\"Linear Regresion AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "TN, FP, FN, TP = confusion_matrix(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN_AIS).ravel()\n",
    "print(\"K Neighbors AIS\")\n",
    "print('True Positive(TP)  = ', TP)\n",
    "print('False Positive(FP) = ', FP)\n",
    "print('True Negative(TN)  = ', TN)\n",
    "print('False Negative(FN) = ', FN)\n",
    "print(\"__________________________________\")\n",
    "\n",
    "\n",
    "print(f\"Geometric Mean Score for Gradient Boosting: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Random Forest: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Logestic Regression: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for K Neighbors: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "\n",
    "print(f\"Geometric Mean Score for Gradient Boosting AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_GB_AIS, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Random Forest AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_RF_AIS, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for Logestic Regression AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_LR_AIS, labels=None, pos_label=1, average='binary',)}\\n\")\n",
    "print(f\"Geometric Mean Score for K Neighbors AIS: \\n{geometric_mean_score(data_test.drop(data_test.columns[0:-1],axis=1), predictions_test_over_KN_AIS, labels=None, pos_label=1, average='binary',)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ea9996f2a91c7d112e626959c304b606e4bf2254e73fec145d965796b2ca69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
